[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Animals In Motion (with answers)",
    "section": "",
    "text": "Preface\nThis website hosts the course materials for the Animals in Motion workshop, part of the inauguaral Neuroinformatics Unit Open Software Week.\nWhether you are attending the workshop in person, or following along the materials at your own pace, be sure to check out Appendix A — Prerequisites.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Animals In Motion (with answers)",
    "section": "",
    "text": "Target audience\n\n\n\nThis course is designed for researchers and students interested in learning about free open-source tools for tracking animal motion from video footage and extracting quantitative descriptions of behaviour from motion tracks.\n\n\n\n\nWorkshop schedule\n\n\nTime\nTopics\n\n\n\n\nDay 1: morning\n1  Introduction  2  Deep learning for computer vision primer\n\n\nDay 1: afternoon\n3  Pose estimation with SLEAP\n\n\nDay 2: morning\n4  Analysing tracks with movement\n\n\nDay 2: afternoon\n5  A mouse’s daily activity log  6  Zebra escape trajectories",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#versions",
    "href": "index.html#versions",
    "title": "Animals In Motion (with answers)",
    "section": "Versions",
    "text": "Versions\nThe latest release version is always available at the following URL:\nhttps://animals-in-motion.neuroinformatics.dev/latest/\nTo view other versions, replace latest in the URL with one of the following version names:\n\n\n\n\n\n\n\nVersion\nDescription\n\n\n\n\ndev\ndevelopment version, corresponding to the main branch\n\n\nv2025.08a2\nalpha version for the 2025.08 release\n\n\nv2025.08a2-answers\nsame as v2025.08a2 but with answers to exercises",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#funding-acknowledgements",
    "href": "index.html#funding-acknowledgements",
    "title": "Animals In Motion (with answers)",
    "section": "Funding & acknowledgements",
    "text": "Funding & acknowledgements\nThe first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK.\nWe thank the Sainsbury Wellcome Centre and the Gatsby Computational Neuroscience Unit for providing facilities for the event.\n\n\n\nLogos of workshop sponsors",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Machine learning methods for motion tracking have transformed a wide range of scientific disciplines—from neuroscience and biomechanics to conservation and ethology. Tools such as DeepLabCut (Mathis et al. 2018) and SLEAP (Pereira et al. 2022) enable researchers to track animal movements in video recordings with impressive accuracy, without the need for physical markers.\nHowever, the variety of available tools can be overwhelming (Luxem et al. 2023). It’s often unclear which tool is best suited to a given application, or how to get started. We’ll provide an overview of the approaches used for quantifying animal behaviour, and we’ll narrow down into Computer Vision (CV) methods for detecting and tracking animals in videos.\n\n\n\n\nLuxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric Yttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023. “Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and Best Practices.” Edited by Denise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018. “DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.” Nature Neuroscience 21 (9): 1281–89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner, Junyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022. “SLEAP: A Deep Learning System for Multi-Animal Pose Tracking.” Nature Methods 19 (4): 486–95. https://doi.org/10.1038/s41592-022-01426-1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-deep-learning.html",
    "href": "02-deep-learning.html",
    "title": "2  Deep learning for computer vision primer",
    "section": "",
    "text": "All you need to know about Deep learning and its applications to computer vision to understand the rest of the course.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Deep learning for computer vision primer</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html",
    "href": "03-sleap-tutorial.html",
    "title": "3  Pose estimation with SLEAP",
    "section": "",
    "text": "Before we proceed, make sure you have installed SLEAP and activated the corresponding conda environment (see prerequisites A.3.2).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html",
    "href": "04-movement-intro.html",
    "title": "4  Analysing tracks with movement",
    "section": "",
    "text": "In this tutorial, we will first introduce the movement package by walking through parts of its documentation.\nAfter that you will be given a set of tasks to complete—using movement to analyse the pose tracks you’ve generated in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html",
    "href": "05-movement-mouse.html",
    "title": "5  A mouse’s daily activity log",
    "section": "",
    "text": "5.1 Import libraries\nIn this case study, we’ll be using the movement package to dive into mouse home cage monitoring data acquired in Smart-Kages and tracked with DeepLabCut. We’ll explore how mouse activity levels fluctuate throughout the day.\nBefore you get started, make sure you’ve set up the animals-in-motion-env environment (refer to prerequisites A.3.3) and are using it to run this code. You’ll also need to download the Smart-Kages.zip archive from Dropbox (see prerequisites A.4) and unzip it.\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\nfrom movement import sample_data\nfrom movement.filtering import filter_by_confidence, rolling_filter\nfrom movement.kinematics import compute_speed\nfrom movement.plots import plot_occupancy\nfrom movement.roi import PolygonOfInterest\nfrom movement.transforms import scale\n\nDownloading data from 'https://gin.g-node.org/neuroinformatics/movement-test-data/raw/master/metadata.yaml' to file '/home/runner/.movement/data/temp_metadata.yaml'.\nSHA256 hash of downloaded file: fc04901d84035f6e6897d71f6bf098517170ebce37c31c6a31e45fbdf83dd64b\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#the-smart-kages-dataset",
    "href": "05-movement-mouse.html#the-smart-kages-dataset",
    "title": "5  A mouse’s daily activity log",
    "section": "5.2 The Smart-Kages dataset",
    "text": "5.2 The Smart-Kages dataset\n\n\n\n\n\n\nAcknowledgement\n\n\n\nThis dataset was kindly shared by Loukia Katsouri from the O’Keefe Lab, with permission to use for this workshop.\n\n\nThe Smart-Kages dataset comprises home cage recordings from two mice, each housed in a specialised Smart-Kage (Ho et al. 2023)—a home cage monitoring system equipped with a camera mounted atop the cage.\nThe camera captures data around the clock at a rate of 2 frames per second, saving a video segment for each hour of the day. A pre-trained DeepLabCut model is subsequently employed to predict 8 keypoints on the mouse’s body.\nLet’s examine the contents of the downloaded data. You will need to specify the path to the unzipped Smart-Kages folder on your machine.\n\n# Replace with the path to the unzipped Smart-Kages folder on your machine\nsmart_kages_path = Path.home() / \".movement\" / \"Smart-Kages\"\n\n# Let's visualise the contents of the folder\nfiles = [f.name for f in smart_kages_path.iterdir()]\nfiles.sort()\nfor file in files:\n    print(file)\n\n.DS_Store\nkage14.nc\nkage14_background.png\nkage17.nc\nkage17_background.png\n\n\nThe tracking data are stored in two.nc (NetCDF) files: kage14 and kage17. NetCDF is an HDF5-based file format that can be natively saved/loaded by the xarray library, and is therefore convenient to use with movement.\nApart from these, we also have two .png files: kage14_background.png and kage17_background.png, which constitute frames extracted from the videos.\nLet’s take a look at them.\n\n\nCode\nkages = [\"kage14\", \"kage17\"]\nimg_paths = [smart_kages_path / f\"{kage}_background.png\" for kage in kages]\nimages = [plt.imread(img_path) for img_path in img_paths]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n\nfor i, img in enumerate(images):\n    axes[i].imshow(img)\n    axes[i].set_title(f\"{kages[i]}\")\n    axes[i].axis(\"off\")\n\n\n\n\n\n\n\n\nFigure 5.1: Top-down camera views of the Smart-Kage habitats\n\n\n\n\n\n\n\n\n\n\n\nQuestions A\n\n\n\n\nWhat objects do you see in the habitat?\nWhat challenges do you anticipate with tracking a mouse in this environment?\nWhat are the trade-offs one has to consider when designing a continuous monitoring system?\n\n\n\n\n\n\n\n\n\nClick to reveal the answers\n\n\n\n\n\n\nThe habitat contains several objects, including the mouse’s nest, a running wheel, a climbing platform, a toilet paper roll, and a corridor with two arms near the top.\nThe presence of these objects is likely to cause occlusions, meaning the mouse or parts of it may frequently be obscured from the camera’s view.\nThe requirements for animal welfare, such as providing a rich environment, often conflict with the needs of the tracking system. While a single black mouse on a white background would be straightforward to track, such conditions would not support the mouse’s well-being over time, nor would they yield naturalistic behavior.\n\n\n\n\nLet’s load and inspect the tracking data:\n\nds_kages = {}  # a dictionary to store kage name -&gt; xarray dataset\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage] = xr.open_dataset(smart_kages_path / f\"{kage}.nc\")\n\nds_kages[\"kage14\"]   # Change to \"kage17\" to inspect the other dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:          (time: 5236793, space: 2, keypoints: 8, individuals: 1)\nCoordinates: (5)\nData variables:\n    position         (time, space, keypoints, individuals) float64 670MB ...\n    confidence       (time, keypoints, individuals) float64 335MB ...\nAttributes: (7)xarray.DatasetDimensions:time: 5236793space: 2keypoints: 8individuals: 1Coordinates: (5)space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U10'snout' 'leftear' ... 'tailbase'array(['snout', 'leftear', 'rightear', 'neck', 'spine1', 'bodycenter',\n       'spine2', 'tailbase'], dtype='&lt;U10')individuals(individuals)&lt;U12'individual_0'array(['individual_0'], dtype='&lt;U12')time(time)datetime64[ns]2024-04-08T13:55:40 ... 2024-05-...array(['2024-04-08T13:55:40.000000000', '2024-04-08T13:55:40.499044000',\n       '2024-04-08T13:55:40.998088000', ..., '2024-05-10T07:59:58.501344000',\n       '2024-05-10T07:59:59.001274000', '2024-05-10T07:59:59.501205000'],\n      shape=(5236793,), dtype='datetime64[ns]')seconds_elapsed(time)float64...[5236793 values with dtype=float64]Data variables: (2)position(time, space, keypoints, individuals)float64...[83788688 values with dtype=float64]confidence(time, keypoints, individuals)float64...[41894344 values with dtype=float64]Indexes: (4)spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['snout', 'leftear', 'rightear', 'neck', 'spine1', 'bodycenter',\n       'spine2', 'tailbase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['individual_0'], dtype='object', name='individuals'))timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-08 13:55:40', '2024-04-08 13:55:40.499044',\n               '2024-04-08 13:55:40.998088', '2024-04-08 13:55:41.497133',\n               '2024-04-08 13:55:41.996177', '2024-04-08 13:55:42.495221',\n               '2024-04-08 13:55:42.994266', '2024-04-08 13:55:43.493310',\n               '2024-04-08 13:55:43.992354', '2024-04-08 13:55:44.491399',\n               ...\n               '2024-05-10 07:59:55.001829', '2024-05-10 07:59:55.501760',\n               '2024-05-10 07:59:56.001691', '2024-05-10 07:59:56.501621',\n               '2024-05-10 07:59:57.001552', '2024-05-10 07:59:57.501482',\n               '2024-05-10 07:59:58.001413', '2024-05-10 07:59:58.501344',\n               '2024-05-10 07:59:59.001274', '2024-05-10 07:59:59.501205'],\n              dtype='datetime64[ns]', name='time', length=5236793, freq=None))Attributes: (7)source_software :DeepLabCutds_type :posesfps :2.0time_unit :datetime64[ns]source_file :/Users/nsirmpilatze/Data/Smart-Kages/kage14/analysis/dlc_output/2024/04/08/kage14_20240408_135536DLC_resnet101_v2Jan17shuffle2_580000.h5kage :kage14kage_start_datetime :2024-04-08T13:55:40\n\n\nWe see that each dataset contains a huge amount of data, but the two datasets are not exactly aligned in time.\n\n\nCode\nstart_times = {\n    name: pd.Timestamp(ds.time.isel(time=0).values)\n    for name, ds in ds_kages.items()\n}\nend_times = {\n    name: pd.Timestamp(ds.time.isel(time=-1).values)\n    for name, ds in ds_kages.items()\n}\n\nfor name in start_times.keys():\n    print(f\"{name}: from {start_times[name]} to {end_times[name]}\")\n\n\nkage14: from 2024-04-08 13:55:40 to 2024-05-10 07:59:59.501205\nkage17: from 2024-04-03 00:00:06 to 2024-05-10 07:59:59.509103",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#datetime-coordinates",
    "href": "05-movement-mouse.html#datetime-coordinates",
    "title": "5  A mouse’s daily activity log",
    "section": "5.3 Datetime Coordinates",
    "text": "5.3 Datetime Coordinates\nYou might notice something interesting about the time coordinates in these xarray datasets: they’re given in datetime64[ns] format, which means they’re precise timestamps expressed in “calendar time”.\nThis is different from what we’ve seen before in other movement datasets, where time coordinates are expressed as seconds elapsed since the start of the video, or “elapsed time”.\n\n\n\n\n\n\nHow did we get these timestamps?\n\n\n\n\n\nMany recording systems can output timestamps for each video frame. In our case, the raw data from the Smart-Kage system included the start datetime of each 1-hour-long video segment and the precise time difference between the start of each segment and every frame within it.\nUsing this information, we were able to reconstruct precise datetime coordinates for all frames throughout the entire experiment. We then concatenated the DeepLabCut predictions from all video segments and assigned the datetime coordinates to the resulting dataset. If you’re interested in the details, you can find the code in the smart-kages-movement GitHub repository.\n\n\n\nUsing “calendar time” is convenient for many applications. For example, we could cross-reference the tracking results against other data sources, such as body weight measurements.\nIt also allows us to easily select time windows by datetime. We will leverage this here to select a time window that’s common to both kages. Note that we discard the last few days because the experimenter introduced some interventions during that time, which are out of scope for this case study.\n\ncommon_start = \"2024-04-09 00:00:00\"\ncommon_end = \"2024-05-07 00:00:00\"\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage] = ds_kages[kage].sel(time=slice(common_start, common_end))\n\nBeyond this ability to select time windows by date and time, we will see many other benefits of using datetime coordinates in the rest of this case study.\nThat said, it’s still useful to also know the total time elapsed since the start of the experiment. In fact, many movement functions will expect “elapsed time” and may not work with datetime coordinates (for now).\nLuckily, it’s easy to convert datetime coordinates to “elapsed time” by simply subtracting the start datetime of the whole experiment from each timestamp.\n\n\nExpand to see how this can be done\nds_14 = ds_kages[\"kage14\"]\n\n# Get the start datetime the experiment in kage14\nexperiment_start = ds_14.time.isel(time=0).data\n\n# Subtract the start datetime from each timestamp\ntime_elapsed = (ds_14.time.data - np.datetime64(experiment_start))\n\n# Convert to seconds\nseconds_elapsed = time_elapsed / pd.Timedelta(\"1s\")\n\n# Assign the seconds_elapsed coordinate to the \"time\" dimension\nds_14 = ds_14.assign_coords(seconds_elapsed=(\"time\", seconds_elapsed))\n\n\nWe’ve pre-computed this for convenience and stored it in a secondary time coordinate called seconds_elapsed.\n\nprint(ds_14.coords[\"time\"].values[:2])\nprint(ds_14.coords[\"seconds_elapsed\"].values[:2])\n\n['2024-04-09T00:00:06.000000000' '2024-04-09T00:00:06.500000000']\n[0.  0.5]\n\n\nWhenever we want to switch to “elapsed time” mode, we can simply set the seconds_elapsed coordinates as the “index” of the time dimension. This means that seconds_elapsed will be used as the primary time coordinate, allowing us to select data by it.\n\nds_14.set_index(time=\"seconds_elapsed\").sel(time=slice(0, 1800))\n\n\n\n\n\n\n\nExercise A\n\n\n\nFor each of the two kages:\n\nPlot the x-axis position of the mouse’s body center over time, for the week starting on April 15th. What do you notice?\nPlot the median confidence of the body center for each day, over the entire duration of the experiment.\n\n\n\n\n\n\n\n\n\nClick to reveal the answers\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 5), sharex=True)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds = ds_kages[kage].squeeze()  # remove redundant \"individuals\" dimension\n    body_center = ds.position.sel(\n        keypoints=\"bodycenter\",\n        time=slice(\"2024-04-15 00:00:00\", \"2024-04-21 23:59:59\"),\n        space=\"x\"\n    )\n    body_center.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body center x\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 5), sharex=True)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds = ds_kages[kage].squeeze()\n    ds_daily = ds.resample(time=\"1D\").median()\n    ds_daily.confidence.sel(keypoints=\"bodycenter\").plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body center\")\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#cleaning-the-data",
    "href": "05-movement-mouse.html#cleaning-the-data",
    "title": "5  A mouse’s daily activity log",
    "section": "5.4 Cleaning the data",
    "text": "5.4 Cleaning the data\nLet’s examine the range of confidence values for each keypoint.\n\n\nCode\nkage = \"kage14\"\nconfidence = ds_kages[kage].confidence.squeeze()\n\nfig, ax = plt.subplots(figsize=(8, 3))\nconfidence.quantile(q=0.25, dim=\"time\").plot.line(\"o--\", color=\"gray\", ax=ax, label=\"25% quantile\")\nconfidence.quantile(q=0.75, dim=\"time\").plot.line(\"o--\", color=\"gray\", ax=ax, label=\"75% quantile\")\nconfidence.median(dim=\"time\").plot.line(\"o-\", color=\"black\", ax=ax, label=\"median\")\n\nax.legend()\nax.set_title(f\"{kage} confidence range\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.2: Confidence range by keypoint\n\n\n\n\n\nIt looks like the “neck”, “bodycenter”, “spine1”, and “spine2” keypoints are the most confidently detected. Let us define a list of “reliable” keypoints for later use. These are all on the mouse’s body.\n\nreliable_keypoints = [\"neck\", \"bodycenter\", \"spine1\", \"spine2\"]\n\nWe can filter out low-confidence predictions.\n\nconfidence_threshold = 0.95\n\nfor kage, ds in ds_kages.items():\n    ds[\"position_filtered\"] = filter_by_confidence(\n        ds.position,\n        ds.confidence,\n        threshold=confidence_threshold,\n    )\n\n\n\n\n\n\n\nExercise B\n\n\n\nLet’s smooth the data with a rolling median filter.\nHint: Remember doing this in Chapter 4 ?\n\n\n\n\n\n\n\n\nClick to reveal the answers\n\n\n\n\n\n\nwindow_size = 3  # frames\n\nfor kage, ds in ds_kages.items():\n    ds[\"position_smoothed\"] = rolling_filter(\n        ds.position_filtered,\n        window_size,\n        statistic=\"median\",\n    )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#plot-the-mouses-speed-over-time",
    "href": "05-movement-mouse.html#plot-the-mouses-speed-over-time",
    "title": "5  A mouse’s daily activity log",
    "section": "5.5 Plot the mouse’s speed over time",
    "text": "5.5 Plot the mouse’s speed over time\nLet’s define a single-point representation of the mouse’s position, which we’ll call the body_centroid. We derive this by taking the mean of the 4 reliable keypoints, using their smoothed positions.\n\nfor kage, ds in ds_kages.items():\n    ds[\"body_centroid\"] = ds.position_filtered.sel(\n        individuals=\"individual_0\",  # the only individual in the dataset\n        keypoints=reliable_keypoints\n    ).mean(dim=\"keypoints\")\n\nNext, we’ll compute the body centroid’s speed in cm/sec via the following steps:\n\nConvert the body centroid position data to cm units using movement.transforms.scale.\nTemporarily switch to “elapsed time” mode, because compute_speed does not (yet) support datetime coordinates.\nCompute the speed in cm/sec\nRestore the original datetime coordinates to the speed data.\n\n\nPIXELS_PER_CM = 10\n\nfor kage, ds in ds_kages.items():\n    # Scale from pixels to cm using a known conversion factor\n    body_centroid_cm = scale(\n        ds.body_centroid, factor=1 / PIXELS_PER_CM, space_unit=\"cm\"\n    )\n\n    # Compute the speed in cm/sec\n    ds[\"body_centroid_speed\"] = compute_speed(\n       body_centroid_cm.set_index(time=\"seconds_elapsed\")  # switch time coords\n    ).assign_coords(time=body_centroid_cm.time)            # restore datetime\n\nds_kages[\"kage14\"].body_centroid_speed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid_speed' (time: 4600675)&gt; Size: 37MB\nnan nan 0.05137 0.001921 0.02156 ... 0.01652 0.0186 0.02228 0.01102 0.03168\nCoordinates: (2)xarray.DataArray'body_centroid_speed'time: 4600675nan nan 0.05137 0.001921 0.02156 ... 0.0186 0.02228 0.01102 0.03168array([       nan,        nan, 0.05137345, ..., 0.02227513, 0.01102056,\n       0.03167816], shape=(4600675,))Coordinates: (2)time(time)datetime64[ns]2024-04-09T00:00:06 ... 2024-05-...array(['2024-04-09T00:00:06.000000000', '2024-04-09T00:00:06.500000000',\n       '2024-04-09T00:00:07.000000000', ..., '2024-05-06T23:59:58.497744000',\n       '2024-05-06T23:59:58.997674000', '2024-05-06T23:59:59.497604000'],\n      shape=(4600675,), dtype='datetime64[ns]')seconds_elapsed(time)float643.627e+04 3.627e+04 ... 2.455e+06array([  36266.      ,   36266.5     ,   36267.      , ..., 2455458.497744,\n       2455458.997674, 2455459.497604], shape=(4600675,))Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-09 00:00:06', '2024-04-09 00:00:06.500000',\n                      '2024-04-09 00:00:07', '2024-04-09 00:00:07.500000',\n                      '2024-04-09 00:00:08', '2024-04-09 00:00:08.500000',\n                      '2024-04-09 00:00:09', '2024-04-09 00:00:09.500000',\n                      '2024-04-09 00:00:10', '2024-04-09 00:00:10.500000',\n               ...\n               '2024-05-06 23:59:54.998233', '2024-05-06 23:59:55.498164',\n               '2024-05-06 23:59:55.998094', '2024-05-06 23:59:56.498024',\n               '2024-05-06 23:59:56.997954', '2024-05-06 23:59:57.497884',\n               '2024-05-06 23:59:57.997814', '2024-05-06 23:59:58.497744',\n               '2024-05-06 23:59:58.997674', '2024-05-06 23:59:59.497604'],\n              dtype='datetime64[ns]', name='time', length=4600675, freq=None))Attributes: (0)\n\n\nLet’s plot the speed over time.\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds_kages[kage].body_centroid_speed.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body centroid\")\n    axes[i].set_ylabel(\"speed (cm/sec)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 5.3: Body centroid speed\n\n\n\n\n\n\n\n\n\n\n\nQuestions B\n\n\n\n\nWhat are potential sources of error in the speed calculation?\nWhat do you notice about the overall speed fluctuations over time? What do you think is the reason for this?\nDo you notice any differences between the two kages? Feel free to “zoom in” on specific time windows to investigate this.\n\n\n\n\n\n\n\n\n\nClick to reveal the answers\n\n\n\n\n\nPotential sources of error in speed calculation: The accuracy of speed computation is heavily reliant on precise position data. Any inaccuracies in predicting keypoint positions (i.e., the discrepancy between actual and estimated positions) can be exacerbated when calculating speed, as it is derived from the differences between consecutive positions. Additionally, we have assumed a fixed conversion factor of 10 pixels per cm. While this value may be accurate for the habitat’s floor, the environment is not entirely flat. It includes objects for climbing, which means the conversion factor will vary depending on the mouse’s vertical position (distance from the camera).\nThe speed seems to fluctuate in a circadian pattern, especially apparent in kage14. This will become clearer if we zoom in on a time window of a few days.\n\n\nCode\ntime_window = slice(\"2024-04-22\", \"2024-04-26\")\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    speed = ds_kages[kage].body_centroid_speed.sel(time=time_window)\n    speed.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body centroid\")\n    axes[i].set_ylabel(\"speed (pixels/sec)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe also note that the circadian pattern is more pronounced in kage14, and seems to be somewhat disrupted in kage17. The mouse in kage17 seems to be more active overall, both during the day and at night.\n\n\n\n\n\n\n\n\n\nA side-note on 3D pose estimation\n\n\n\n\n\nWith only a single top-down view of the mouse, we are limited to 2D pose estimation. This means the estimated keypoint coordinates are simply projections of the true 3D coordinates onto the 2D image plane.\nThis is the most common approach to pose estimation, but it cannot accurately measure true dimensions. Any conversion from pixels to physical units (e.g. centimetres) will be imprecise, and sometimes significantly so.\nThis limitation can be overcome by using multiple cameras from different viewpoints and performing 3D pose estimation. There are two main markerless approaches:\n\nThe first approach is to do ‘regular’ 2D pose estimation in each camera view, then triangulate across camera views to estimate 3D pose. The triangulation relies on known parameters about the cameras and their relative positions and orientations. Anipose (Karashchuk et al. 2021) is a popular open-source toolkit that implements this approach.\n\n\n\nSource: Pereira, Shaevitz, and Murthy (2020)\n\n\nThe second approach, implemented in DANNCE (Dunn et al. 2021), is to use a fully 3D convolutional neural network (CNN) that can learn about 3D image features and how cameras and landmarks relate to one another in 3D space.\n\n\n\nSource: https://github.com/spoonsso/dannce\n\n\n\nSome prompts for discussion:\n\nWhat are the pros and cons of 2D vs 3D markerless pose estimation?\nIn which scenarios would you prefer one over the other?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#sec-actograms",
    "href": "05-movement-mouse.html#sec-actograms",
    "title": "5  A mouse’s daily activity log",
    "section": "5.6 Plot actograms",
    "text": "5.6 Plot actograms\nAn actogram is a visualisation of the mouse’s activity level over time.\nAs a measure of activity we’ll use the cumulative distance traversed by the mouse’s body centroid in a given time bin—in this case, 10 minutes. Since we have already computed the speed (cm/sec), we can multiply that by the time bin duration in seconds to get the distance (cm).\n\nfor kage in [\"kage14\", \"kage17\"]:\n    time_diff = ds_kages[kage].coords[\"seconds_elapsed\"].diff(dim=\"time\")\n    ds_kages[kage][\"distance\"] = ds_kages[kage].body_centroid_speed * time_diff\n\nThen we can sum the distance traversed in each time bin to get the activity level.\n\ntime_bin_minutes = 10\ntime_bin_duration = pd.Timedelta(f\"{time_bin_minutes}min\")\n\nfig, ax = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 5), sharex=True, sharey=True\n)\n\nactivity_dict = {}  # Dictionary to store the activity levels for each kage\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    activity = ds_kages[kage].distance.resample(time=time_bin_duration).sum()\n    activity.plot.line(ax=ax[i])\n    ax[i].set_title(f\"{kage} activity\")\n    ax[i].set_ylabel(\"distance (cm)\")\n    ax[i].set_xlabel(\"time\")\n\n    activity_dict[kage] = activity\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 5.4: Activity over time\n\n\n\n\n\nTo make any circadian patterns more apparent, we will stack days vertically and indicate the light cycle with gray areas. For this particular experiment, the light cycle is as follows:\n\nlights off at 9:30\ndawn at 20:30\nlights on at 21:30\n\n\n# Define light cycle (in minutes since midnight)\nlights_off = 9 * 60 + 30  # 9:30 AM in minutes\ndawn = 20 * 60 + 30       # 8:30 PM in minutes  \nlights_on = 21 * 60 + 30  # 9:30 PM in minutes\n\nn_bins_in_day = int(24 * 60 / time_bin_minutes)\n\nactogram_dict = {}  # Dictionary to store the 2D actogram for each kage\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    activity = activity_dict[kage]\n    days = list(activity.groupby(\"time.date\").groups.keys())\n\n    # Create an empty 2D actogram with dims (date, time_bin)\n    actogram = xr.DataArray(\n        np.zeros((len(days), n_bins_in_day)),\n        dims=[\"date\", \"time_of_day\"],\n        coords={\n            \"date\": days,\n            \"time_of_day\": np.arange(\n                time_bin_minutes/2, 24 * 60, time_bin_minutes\n            )\n        },\n    )\n    # Populate 2D actogram per day\n    for date, day_activity in activity.groupby(\"time.date\"):\n        actogram.loc[dict(date=date)] = day_activity.values\n\n    # Store the actogram in the dictionary for later use\n    actogram_dict[kage] = actogram\n\nactogram_dict[\"kage14\"]  # Replace with kage17 to see the actogram for kage17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (date: 28, time_of_day: 144)&gt; Size: 32kB\n21.94 12.34 15.7 40.82 51.96 359.7 337.1 ... 59.47 65.09 69.83 41.08 50.58 95.62\nCoordinates: (2)xarray.DataArraydate: 28time_of_day: 14421.94 12.34 15.7 40.82 51.96 359.7 ... 65.09 69.83 41.08 50.58 95.62array([[ 21.94353274,  12.3359559 ,  15.69606955, ..., 370.01643336,\n        148.41509776,  43.63468815],\n       [ 24.34439421,  28.70382676,  35.21278474, ...,  37.0943919 ,\n         32.72694742, 170.30008811],\n       [165.96110319,  22.25839305,  22.3546828 , ...,  13.99343428,\n         10.45916989,  17.76600485],\n       ...,\n       [ 17.13844639,  19.77411278,  42.93937937, ...,  10.3900106 ,\n         21.68248825, 166.8786525 ],\n       [268.03459651,  65.77269862,  72.07467688, ..., 211.74754295,\n         71.80358134,  57.01412942],\n       [ 50.11294109,  30.36584632,  91.0799254 , ...,  41.08254947,\n         50.5819582 ,  95.6220443 ]], shape=(28, 144))Coordinates: (2)date(date)object2024-04-09 ... 2024-05-06array([datetime.date(2024, 4, 9), datetime.date(2024, 4, 10),\n       datetime.date(2024, 4, 11), datetime.date(2024, 4, 12),\n       datetime.date(2024, 4, 13), datetime.date(2024, 4, 14),\n       datetime.date(2024, 4, 15), datetime.date(2024, 4, 16),\n       datetime.date(2024, 4, 17), datetime.date(2024, 4, 18),\n       datetime.date(2024, 4, 19), datetime.date(2024, 4, 20),\n       datetime.date(2024, 4, 21), datetime.date(2024, 4, 22),\n       datetime.date(2024, 4, 23), datetime.date(2024, 4, 24),\n       datetime.date(2024, 4, 25), datetime.date(2024, 4, 26),\n       datetime.date(2024, 4, 27), datetime.date(2024, 4, 28),\n       datetime.date(2024, 4, 29), datetime.date(2024, 4, 30),\n       datetime.date(2024, 5, 1), datetime.date(2024, 5, 2),\n       datetime.date(2024, 5, 3), datetime.date(2024, 5, 4),\n       datetime.date(2024, 5, 5), datetime.date(2024, 5, 6)], dtype=object)time_of_day(time_of_day)float645.0 15.0 ... 1.425e+03 1.435e+03array([   5.,   15.,   25.,   35.,   45.,   55.,   65.,   75.,   85.,   95.,\n        105.,  115.,  125.,  135.,  145.,  155.,  165.,  175.,  185.,  195.,\n        205.,  215.,  225.,  235.,  245.,  255.,  265.,  275.,  285.,  295.,\n        305.,  315.,  325.,  335.,  345.,  355.,  365.,  375.,  385.,  395.,\n        405.,  415.,  425.,  435.,  445.,  455.,  465.,  475.,  485.,  495.,\n        505.,  515.,  525.,  535.,  545.,  555.,  565.,  575.,  585.,  595.,\n        605.,  615.,  625.,  635.,  645.,  655.,  665.,  675.,  685.,  695.,\n        705.,  715.,  725.,  735.,  745.,  755.,  765.,  775.,  785.,  795.,\n        805.,  815.,  825.,  835.,  845.,  855.,  865.,  875.,  885.,  895.,\n        905.,  915.,  925.,  935.,  945.,  955.,  965.,  975.,  985.,  995.,\n       1005., 1015., 1025., 1035., 1045., 1055., 1065., 1075., 1085., 1095.,\n       1105., 1115., 1125., 1135., 1145., 1155., 1165., 1175., 1185., 1195.,\n       1205., 1215., 1225., 1235., 1245., 1255., 1265., 1275., 1285., 1295.,\n       1305., 1315., 1325., 1335., 1345., 1355., 1365., 1375., 1385., 1395.,\n       1405., 1415., 1425., 1435.])Indexes: (2)datePandasIndexPandasIndex(Index([2024-04-09, 2024-04-10, 2024-04-11, 2024-04-12, 2024-04-13, 2024-04-14,\n       2024-04-15, 2024-04-16, 2024-04-17, 2024-04-18, 2024-04-19, 2024-04-20,\n       2024-04-21, 2024-04-22, 2024-04-23, 2024-04-24, 2024-04-25, 2024-04-26,\n       2024-04-27, 2024-04-28, 2024-04-29, 2024-04-30, 2024-05-01, 2024-05-02,\n       2024-05-03, 2024-05-04, 2024-05-05, 2024-05-06],\n      dtype='object', name='date'))time_of_dayPandasIndexPandasIndex(Index([   5.0,   15.0,   25.0,   35.0,   45.0,   55.0,   65.0,   75.0,   85.0,\n         95.0,\n       ...\n       1345.0, 1355.0, 1365.0, 1375.0, 1385.0, 1395.0, 1405.0, 1415.0, 1425.0,\n       1435.0],\n      dtype='float64', name='time_of_day', length=144))Attributes: (0)\n\n\nLet’s now visualise the actograms, with the light cycle marked.\n\n\nCode\nmax_activity = max(\n    actogram_dict[kage].max().values for kage in [\"kage14\", \"kage17\"]\n)\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 8), sharex=True, sharey=True,\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    actogram = actogram_dict[kage]\n\n    # Plot the actogram\n    ax = axes[i]\n    actogram.plot(\n        ax=ax, yincrease=False, cmap=\"binary\", vmin=0, vmax=max_activity,\n    )\n\n    # Assign x-tick labels every 4 hours formatted as HH:MM\n    ax.set_xticks(np.arange(0, 24 * 60 + 1, 4 * 60))\n    ax.set_xticklabels([f\"{i:02d}:00\" for i in np.arange(0, 25, 4)])\n\n    # Mark light cycle\n    ax.axvline(lights_off, color=\"red\", alpha=0.7, lw=2, linestyle=\"-\", label=\"lights off\")\n    ax.axvline(dawn, color=\"blue\", alpha=0.7, lw=2, linestyle=\"--\", label=\"dawn\")\n    ax.axvline(lights_on, color=\"blue\", alpha=0.7, lw=2, linestyle=\"-\", label=\"lights on\")\n    ax.legend()\n\n    # Set title and axis labels\n    ax.set_title(f\"{kage} actogram\")\n    ax.set_xlabel(\"Time of day\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.5: Actograms\n\n\n\n\n\n\n\n\n\n\n\nExercise C\n\n\n\n\nHow would you describe the observed differences between the two mice?\nCompute the mean activity profile (across days) for each mouse.\nPlot the mean activity profile for each mouse, with the light cycle marked.\n\nHint: Start with the actogram_dict dictionary and also make use of the lights_off and lights_on variables defined above.\n\n\n\n\n\n\n\n\nClick to reveal the answers\n\n\n\n\n\nThe mouse in kage17 show higher overall activity levels, stays constantly active during dark periods and also exhibits many activity bursts during the day (at least more than kage14 does).\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Mark light cycle\nax.axvspan(lights_off, lights_on, color=\"0.8\", label=\"Dark period\")\n\nfor kage, actogram in actogram_dict.items():    \n    actogram.mean(dim=\"date\").plot.line(ax=ax, label=kage, lw=3)\n\nax.set_title(f\"Mean daily activity profile\")\nax.set_ylabel(f\"Activity\")\nax.set_xlabel(\"Time of day\")\n\n# Assign x-tick labels every 4 hours formatted as HH:MM\nax.set_xticks(np.arange(0, 24 * 60 + 1, 4 * 60))\nax.set_xticklabels([f\"{i:02d}:00\" for i in np.arange(0, 25, 4)])\n\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.6: Mean daily activity profiles",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#space-occupancy",
    "href": "05-movement-mouse.html#space-occupancy",
    "title": "5  A mouse’s daily activity log",
    "section": "5.7 Space occupancy",
    "text": "5.7 Space occupancy\nApart from quantifying how active the mice were over time, we might also be interested in which parts of the habitat they tend to frequent.\nmovement provides a plot_occupancy() function to help us visualise the space occupancy.\n\nfig, axes = plt.subplots(\n    nrows=1, ncols=2, figsize=(8, 4), sharex=True, sharey=True\n)\n\nplt.suptitle(\"Body centroid occupancy (log scale)\")\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    height, width = img.shape[:2]\n\n    axes[i].imshow(img)\n    plot_occupancy(\n        # Setting the time coordinates to \"elapsed time\" is necessary\n        # for the log scale to work properly.\n        ds_kages[kage].body_centroid.set_index(time=\"seconds_elapsed\"),\n        ax=axes[i],\n        cmap=\"turbo\",\n        norm=\"log\",  # log scale the colormap\n        vmax=10**6,\n        alpha=0.6,   # some transparency\n    )\n    # Make axes match the image dimensions\n    axes[i].set_ylim([height - 1, 0])\n    axes[i].set_xlim([0, width])\n    axes[i].set_title(kage)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 5.7: Occupancy heatmaps\n\n\n\n\n\nWe see some clear hotspots, such as the nest and climbing platform. But not all hotspots are created equal. For example, the nest should be occupied when the mouse is stationary but not when it is moving.\nTo investigate this, let’s choose an arbitrary speed limit of 4 cm/sec, below which we consider the mouse to be stationary.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 4))\n\nspeed_threshold = 4\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage].body_centroid_speed.plot.hist(\n        ax=ax, bins=50, alpha=0.5, label=kage, histtype=\"step\",\n    )\n\nax.axvline(speed_threshold, linestyle=\"--\", color=\"red\", label=\"Speed threshold\")\n\nax.set_title(\"Body centroid speed\")\nax.set_xlabel(\"Speed (cm/sec)\")\nax.set_ylabel(\"Count\")\nax.set_yscale(\"log\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.8: Body centroid speed histogram (log scale)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe use the log scale because speeds tend to be exponentially distributed, i.e. the mouse spends far more time at low speeds than at high speeds. Try commenting out the ax.set_yscale(\"log\") line and see what happens.\n\n\nWe will generate separate occupancy heatmaps for when the mouse is stationary vs moving. We can do this by masking with where().\n\nstationary_mask = ds_kages[\"kage14\"].body_centroid_speed &lt; 4\nstationary_mask\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid_speed' (time: 4600675)&gt; Size: 5MB\nFalse False True True True True True True ... True True True True True True True\nCoordinates: (2)xarray.DataArray'body_centroid_speed'time: 4600675False False True True True True True ... True True True True True Truearray([False, False,  True, ...,  True,  True,  True], shape=(4600675,))Coordinates: (2)time(time)datetime64[ns]2024-04-09T00:00:06 ... 2024-05-...array(['2024-04-09T00:00:06.000000000', '2024-04-09T00:00:06.500000000',\n       '2024-04-09T00:00:07.000000000', ..., '2024-05-06T23:59:58.497744000',\n       '2024-05-06T23:59:58.997674000', '2024-05-06T23:59:59.497604000'],\n      shape=(4600675,), dtype='datetime64[ns]')seconds_elapsed(time)float643.627e+04 3.627e+04 ... 2.455e+06array([  36266.      ,   36266.5     ,   36267.      , ..., 2455458.497744,\n       2455458.997674, 2455459.497604], shape=(4600675,))Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-09 00:00:06', '2024-04-09 00:00:06.500000',\n                      '2024-04-09 00:00:07', '2024-04-09 00:00:07.500000',\n                      '2024-04-09 00:00:08', '2024-04-09 00:00:08.500000',\n                      '2024-04-09 00:00:09', '2024-04-09 00:00:09.500000',\n                      '2024-04-09 00:00:10', '2024-04-09 00:00:10.500000',\n               ...\n               '2024-05-06 23:59:54.998233', '2024-05-06 23:59:55.498164',\n               '2024-05-06 23:59:55.998094', '2024-05-06 23:59:56.498024',\n               '2024-05-06 23:59:56.997954', '2024-05-06 23:59:57.497884',\n               '2024-05-06 23:59:57.997814', '2024-05-06 23:59:58.497744',\n               '2024-05-06 23:59:58.997674', '2024-05-06 23:59:59.497604'],\n              dtype='datetime64[ns]', name='time', length=4600675, freq=None))Attributes: (0)\n\n\nA “mask” is just a boolean array of the same shape as the original data. It’s True where the condition is met, and False otherwise.\n\nstationary_position = ds_kages[\"kage14\"].body_centroid.where(stationary_mask)\nstationary_position\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid' (time: 4600675, space: 2)&gt; Size: 74MB\nnan nan nan nan 198.7 177.6 198.7 ... 44.09 128.6 43.99 128.7 43.94 128.5 44.0\nCoordinates: (3)\nAttributes: (1)xarray.DataArray'body_centroid'time: 4600675space: 2nan nan nan nan 198.7 177.6 ... 128.6 43.99 128.7 43.94 128.5 44.0array([[         nan,          nan],\n       [         nan,          nan],\n       [198.68329239, 177.6133194 ],\n       ...,\n       [128.63087654,  43.9933548 ],\n       [128.66922379,  43.94464111],\n       [128.520895  ,  44.00013161]], shape=(4600675, 2))Coordinates: (3)space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')time(time)datetime64[ns]2024-04-09T00:00:06 ... 2024-05-...array(['2024-04-09T00:00:06.000000000', '2024-04-09T00:00:06.500000000',\n       '2024-04-09T00:00:07.000000000', ..., '2024-05-06T23:59:58.497744000',\n       '2024-05-06T23:59:58.997674000', '2024-05-06T23:59:59.497604000'],\n      shape=(4600675,), dtype='datetime64[ns]')seconds_elapsed(time)float643.627e+04 3.627e+04 ... 2.455e+06array([  36266.      ,   36266.5     ,   36267.      , ...,\n       2455458.497744, 2455458.997674, 2455459.497604], shape=(4600675,))Indexes: (2)spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-09 00:00:06', '2024-04-09 00:00:06.500000',\n                      '2024-04-09 00:00:07', '2024-04-09 00:00:07.500000',\n                      '2024-04-09 00:00:08', '2024-04-09 00:00:08.500000',\n                      '2024-04-09 00:00:09', '2024-04-09 00:00:09.500000',\n                      '2024-04-09 00:00:10', '2024-04-09 00:00:10.500000',\n               ...\n               '2024-05-06 23:59:54.998233', '2024-05-06 23:59:55.498164',\n               '2024-05-06 23:59:55.998094', '2024-05-06 23:59:56.498024',\n               '2024-05-06 23:59:56.997954', '2024-05-06 23:59:57.497884',\n               '2024-05-06 23:59:57.997814', '2024-05-06 23:59:58.497744',\n               '2024-05-06 23:59:58.997674', '2024-05-06 23:59:59.497604'],\n              dtype='datetime64[ns]', name='time', length=4600675, freq=None))Attributes: (1)log :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-08-09 19:14:38.700426\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 4600675, keypoints: 8, individuals: 1)&gt; Size: 294MB\\narray([[[         nan],\\n        [         nan],\\n        ...,\\n        [         nan],\\n        [         nan]],\\n\\n       [[1.728711e-02],\\n        [9.850310e-01],\\n        ...,\\n        [9.821828e-01],\\n        [8.725649e-04]],\\n\\n       ...,\\n\\n       [[9.718522e-01],\\n        [9.982640e-01],\\n        ...,\\n        [9.880463e-01],\\n        [9.681368e-01]],\\n\\n       [[9.761935e-01],\\n        [9.980691e-01],\\n        ...,\\n        [9.910011e-01],\\n        [9.745104e-01]]], shape=(4600675, 8, 1))\\nCoordinates: (4)\",\n    \"threshold\": \"0.95\",\n    \"print_report\": \"False\"\n  }\n]\n\n\nWe see that the where() method returns a new xarray.DataArray with the same dimensions as the original, but with the data values replaced by NaN where the condition (mask) is False.\nUsing this approach we can generate separate the body centroid position arrays into states where the mouse is stationary vs moving, and then plot the occupancy heatmaps for each state.\n\n\nCode\nfig, axes = plt.subplots(\n    nrows=2, ncols=2, figsize=(8, 8), sharex=True, sharey=True\n)\n\nplt.suptitle(\"Body centroid occupancy (log scale)\")\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    height, width = img.shape[:2]\n\n    masks = {\n        \"stationary\": ds_kages[kage].body_centroid_speed &lt; 4,\n        \"moving\": ds_kages[kage].body_centroid_speed &gt;= 4,\n    }\n\n    for j, (mask_name, mask) in enumerate(masks.items()):\n        ax = axes[i, j]\n        ax.imshow(img)\n        plot_occupancy(\n            ds_kages[kage].body_centroid.where(mask).set_index(time=\"seconds_elapsed\"),\n            ax=ax,\n            cmap=\"turbo\",\n            norm=\"log\",\n            vmax=10**6,\n            alpha=0.6,\n        )\n        ax.set_title(f\"{kage} {mask_name}\")\n        ax.set_ylim([height - 1, 0])\n        ax.set_xlim([0, width])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.9: Occupancy heatmaps (stationary vs active)\n\n\n\n\n\nWe see some expected patterns like the nest being a “hotspot” during stationary periods, and going “dark” during active periods. But we also see some puzzling patterns: for example, the running wheel is occupied during both periods, including when the mouse is “stationary”.\nIs that because the mouse appears to be stationary to the camera as it’s running “in-place” on the wheel (like on a treadmill)? Or maybe the mouse spends some of its downtime resting on the wheel?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#region-of-interest-occupancy",
    "href": "05-movement-mouse.html#region-of-interest-occupancy",
    "title": "5  A mouse’s daily activity log",
    "section": "5.8 Region of interest occupancy",
    "text": "5.8 Region of interest occupancy\nLet us precisely measure running wheel occupancy, i.e. the proportion of time the mouse spends on the running wheel. Here we’ll use movement’s functionality for defining regions of interest (ROIs).\nLet us create a circular “running wheel” ROI for each of the two kages.\n\ncentres = {\n    \"kage14\": np.array([145, 260]),  # (x, y)\n    \"kage17\": np.array([385, 210]),\n}\nradius = 70\n\n# Create a unit circle\nn_points = 32\nangles = np.linspace(0, 2 * np.pi, n_points)\nunit_circle = np.column_stack([np.cos(angles), np.sin(angles)])\n\n# Create ROIs by scaling and shifting the unit circle\nrois = {}\nfor kage in [\"kage14\", \"kage17\"]:\n    points = centres[kage] + radius * unit_circle\n    roi = PolygonOfInterest(points, name=f\"{kage} running wheel\")\n    rois[kage] = roi\n\n\n\n\n\n\n\nNote\n\n\n\nAdmittedly, this is not the most precise or convenient way to define the running wheel ROI. It would be better to directly draw shapes on the video frames.\nWe are actively working on a widget in napari that will enable this. Stay tuned for updates in movement by joining the “movement” channel on Zulip.\n\n\nNow that we have the ROIs defined as PolygonOfInterest objects, we can use some of their built-in methods.\nFor example, we can use .plot() to visualise the ROIs and verify that they are roughly in the right place.\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 8))\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    ax[i].imshow(img)\n    rois[kage].plot(ax=ax[i], alpha=0.25, facecolor=\"red\")\n    ax[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.10: Running wheel ROIs\n\n\n\n\n\nWe can also use .contains_point() to check if a point is inside the ROI. If we pass it a whole xarray.DataArray of points positions, e.g. the positions of the mouse’s body centroid over time, the check is performed for each point in the array.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code cell will take a while to run, probably a few minutes. That’s because we have a large amount of data and the .contains_point() method is not fully optimised yet.\nIf you are an experienced Python programmer this could be a cool project for the hackathon.\n\n\n\nroi_occupancy = {\n    kage: rois[kage].contains_point(ds_kages[kage].body_centroid)\n    for kage in [\"kage14\", \"kage17\"]\n}\n\nroi_occupancy[\"kage14\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid' (time: 4600675)&gt; Size: 5MB\nFalse False False False False False ... False False False False False False\nCoordinates: (2)\nAttributes: (1)xarray.DataArray'body_centroid'time: 4600675False False False False False False ... False False False False Falsearray([False, False, False, ..., False, False, False], shape=(4600675,))Coordinates: (2)time(time)datetime64[ns]2024-04-09T00:00:06 ... 2024-05-...array(['2024-04-09T00:00:06.000000000', '2024-04-09T00:00:06.500000000',\n       '2024-04-09T00:00:07.000000000', ..., '2024-05-06T23:59:58.497744000',\n       '2024-05-06T23:59:58.997674000', '2024-05-06T23:59:59.497604000'],\n      shape=(4600675,), dtype='datetime64[ns]')seconds_elapsed(time)float643.627e+04 3.627e+04 ... 2.455e+06array([  36266.      ,   36266.5     ,   36267.      , ..., 2455458.497744,\n       2455458.997674, 2455459.497604], shape=(4600675,))Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-09 00:00:06', '2024-04-09 00:00:06.500000',\n                      '2024-04-09 00:00:07', '2024-04-09 00:00:07.500000',\n                      '2024-04-09 00:00:08', '2024-04-09 00:00:08.500000',\n                      '2024-04-09 00:00:09', '2024-04-09 00:00:09.500000',\n                      '2024-04-09 00:00:10', '2024-04-09 00:00:10.500000',\n               ...\n               '2024-05-06 23:59:54.998233', '2024-05-06 23:59:55.498164',\n               '2024-05-06 23:59:55.998094', '2024-05-06 23:59:56.498024',\n               '2024-05-06 23:59:56.997954', '2024-05-06 23:59:57.497884',\n               '2024-05-06 23:59:57.997814', '2024-05-06 23:59:58.497744',\n               '2024-05-06 23:59:58.997674', '2024-05-06 23:59:59.497604'],\n              dtype='datetime64[ns]', name='time', length=4600675, freq=None))Attributes: (1)log :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-08-09 19:14:38.700426\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 4600675, keypoints: 8, individuals: 1)&gt; Size: 294MB\\narray([[[         nan],\\n        [         nan],\\n        ...,\\n        [         nan],\\n        [         nan]],\\n\\n       [[1.728711e-02],\\n        [9.850310e-01],\\n        ...,\\n        [9.821828e-01],\\n        [8.725649e-04]],\\n\\n       ...,\\n\\n       [[9.718522e-01],\\n        [9.982640e-01],\\n        ...,\\n        [9.880463e-01],\\n        [9.681368e-01]],\\n\\n       [[9.761935e-01],\\n        [9.980691e-01],\\n        ...,\\n        [9.910011e-01],\\n        [9.745104e-01]]], shape=(4600675, 8, 1))\\nCoordinates: (4)\",\n    \"threshold\": \"0.95\",\n    \"print_report\": \"False\"\n  }\n]\n\n\nThe ROI occupancy data is a boolean array which is True when a point (in this case the mouse’s body centroid at each time point) is inside the ROI, and False otherwise.\n\n\nCode\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ax = axes[i]\n    roi_occupancy[kage].sel(time=slice(None, \"2024-04-09 23:59:59\")).plot.line(\n        \"-\", ax=ax, lw=0.1\n    )\n    ax.set_title(kage)\n    ax.set_ylabel(\"Occupancy\")\n    ax.set_yticks([0, 1])\n    ax.set_yticklabels([\"False\", \"True\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.11: Running wheel occupancy during April 9, 2024\n\n\n\n\n\nWe can also compute the % of time the mouse spends on the running wheel.\n\nfor kage in [\"kage14\", \"kage17\"]:\n    # Count the number of True values in the array\n    on_wheel = roi_occupancy[kage].sum(dim=\"time\").values\n\n    # Count the number of non-NaN values in the array\n    total_time = roi_occupancy[kage].notnull().sum(dim=\"time\").values\n\n    # Compute the percentage of time spent on the running wheel\n    pct_on_wheel = float(100 * on_wheel / total_time)\n    print(f\"{kage} spends {pct_on_wheel:.1f}% of its time on the running wheel\")\n\nkage14 spends 7.7% of its time on the running wheel\nkage17 spends 5.6% of its time on the running wheel\n\n\nBut how does the running wheel occupancy relate to the light cycle? We can segment the ROI occupancy data into dark and light periods for each day and compute the % of time spent on the running wheel during each period.\n\n\nCode\ndays = list(\n    roi_occupancy[\"kage14\"].dropna(dim=\"time\").groupby(\"time.date\").groups.keys()\n)\nn_days = len(days)\n\n# Create a new DataArray of NaNs with shape (kage, date, period)\ndaily_occupancy = xr.DataArray(\n    np.full((2, n_days, 2), np.nan),\n    dims=[\"kage\", \"date\", \"period\"],\n    coords={\n        \"kage\": [\"kage14\", \"kage17\"],\n        \"date\": days,\n        \"period\": [\"dark\", \"light\"]\n    },\n)\n\nfor kage in [\"kage14\", \"kage17\"]:\n    for date, day_ds in roi_occupancy[kage].dropna(dim=\"time\").groupby(\"time.date\"):\n        dark_period = slice(f\"{date} 09:30:00\", f\"{date} 21:30:00\")\n        light_period1 = slice(f\"{date} 00:00:00\", f\"{date} 09:30:00\")\n        light_period2 = slice(f\"{date} 21:30:00\", f\"{date} 23:59:59\")\n\n        n_total_frames = day_ds.sizes[\"time\"]\n        n_dark_frames = day_ds.sel(time=dark_period).sizes[\"time\"]\n        n_light_frames = n_total_frames - n_dark_frames\n        if n_dark_frames == 0 or n_light_frames == 0:\n            continue\n\n        dark_occupancy = 100 * day_ds.sel(time=dark_period).sum() / n_dark_frames\n        light_occupancy = 100 * (\n            day_ds.sel(time=light_period1).sum() +\n            day_ds.sel(time=light_period2).sum()\n        ) / n_light_frames\n\n        daily_occupancy.loc[dict(kage=kage, date=date, period=\"dark\")] = dark_occupancy\n        daily_occupancy.loc[dict(kage=kage, date=date, period=\"light\")] = light_occupancy\n\n\n\ndaily_occupancy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (kage: 2, date: 27, period: 2)&gt; Size: 864B\n18.14 0.2826 14.61 0.2203 15.77 0.04173 ... 0.5738 6.061 1.316 5.416 0.7154\nCoordinates: (3)xarray.DataArraykage: 2date: 27period: 218.14 0.2826 14.61 0.2203 15.77 ... 0.5738 6.061 1.316 5.416 0.7154array([[[1.81372118e+01, 2.82601693e-01],\n        [1.46142966e+01, 2.20251550e-01],\n        [1.57747687e+01, 4.17323565e-02],\n        [1.75027535e+01, 5.68017156e-02],\n        [1.67947782e+01, 1.13608699e-01],\n        [2.10938768e+01, 2.56205150e-01],\n        [2.23020915e+01, 2.31851800e-02],\n        [2.50912969e+01, 4.63703601e-02],\n        [1.92125765e+01, 2.15622174e-01],\n        [2.01472464e+01, 1.66929426e-01],\n        [1.05742703e+01, 2.08664201e-01],\n        [2.04822629e+01, 1.84309362e-01],\n        [1.75749145e+01, 1.85485741e-01],\n        [2.30468705e+01, 1.46058168e-01],\n        [2.47936662e+01, 2.26050264e-01],\n        [2.28372379e+01, 5.57584188e-01],\n        [1.33741396e+01, 2.62090575e-01],\n        [1.03754883e+01, 1.33661013e+00],\n        [7.56987353e+00, 0.00000000e+00],\n        [1.00954015e+01, 1.78171912e+00],\n...\n        [4.60557584e+00, 4.73054448e-01],\n        [2.87145126e+00, 5.88944537e-01],\n        [           nan,            nan],\n        [1.46573008e+01, 7.41985972e-02],\n        [1.27999444e+01, 6.71210962e-01],\n        [1.15901871e+01, 8.17476606e-01],\n        [1.23581439e+01, 5.37939830e-01],\n        [1.71964849e+01, 6.28383940e-01],\n        [1.50241109e+01, 3.40490157e+00],\n        [1.47872957e+01, 3.69855072e-01],\n        [6.47423636e+00, 6.79404536e-01],\n        [8.67453399e+00, 1.23047849e+01],\n        [1.06930486e+01, 5.72720422e-01],\n        [9.61627637e+00, 1.91286605e-01],\n        [1.33663309e+01, 1.47021311e+00],\n        [9.25234728e+00, 2.44400130e+00],\n        [1.42090376e+01, 6.09798511e-01],\n        [7.64156958e+00, 5.73846511e-01],\n        [6.06127346e+00, 1.31592677e+00],\n        [5.41580692e+00, 7.15354025e-01]]])Coordinates: (3)kage(kage)&lt;U6'kage14' 'kage17'array(['kage14', 'kage17'], dtype='&lt;U6')date(date)object2024-04-09 ... 2024-05-06array([datetime.date(2024, 4, 9), datetime.date(2024, 4, 10),\n       datetime.date(2024, 4, 11), datetime.date(2024, 4, 13),\n       datetime.date(2024, 4, 14), datetime.date(2024, 4, 15),\n       datetime.date(2024, 4, 16), datetime.date(2024, 4, 17),\n       datetime.date(2024, 4, 18), datetime.date(2024, 4, 19),\n       datetime.date(2024, 4, 20), datetime.date(2024, 4, 21),\n       datetime.date(2024, 4, 22), datetime.date(2024, 4, 23),\n       datetime.date(2024, 4, 24), datetime.date(2024, 4, 25),\n       datetime.date(2024, 4, 26), datetime.date(2024, 4, 27),\n       datetime.date(2024, 4, 28), datetime.date(2024, 4, 29),\n       datetime.date(2024, 4, 30), datetime.date(2024, 5, 1),\n       datetime.date(2024, 5, 2), datetime.date(2024, 5, 3),\n       datetime.date(2024, 5, 4), datetime.date(2024, 5, 5),\n       datetime.date(2024, 5, 6)], dtype=object)period(period)&lt;U5'dark' 'light'array(['dark', 'light'], dtype='&lt;U5')Indexes: (3)kagePandasIndexPandasIndex(Index(['kage14', 'kage17'], dtype='object', name='kage'))datePandasIndexPandasIndex(Index([2024-04-09, 2024-04-10, 2024-04-11, 2024-04-13, 2024-04-14, 2024-04-15,\n       2024-04-16, 2024-04-17, 2024-04-18, 2024-04-19, 2024-04-20, 2024-04-21,\n       2024-04-22, 2024-04-23, 2024-04-24, 2024-04-25, 2024-04-26, 2024-04-27,\n       2024-04-28, 2024-04-29, 2024-04-30, 2024-05-01, 2024-05-02, 2024-05-03,\n       2024-05-04, 2024-05-05, 2024-05-06],\n      dtype='object', name='date'))periodPandasIndexPandasIndex(Index(['dark', 'light'], dtype='object', name='period'))Attributes: (0)\n\n\nLet’s visualise the % of time spent on the running wheel during the light and dark periods of each day.\n\n\nCode\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n\nmax_occupancy = daily_occupancy.max()\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    daily_occupancy.sel(kage=kage).plot(\n        ax=axes[i], lw=3, vmax=daily_occupancy.max(), yincrease=False\n    )\n\n    axes[i].set_title(f\"{kage} running wheel occupancy\")\n    axes[i].set_xticks([0.25, 0.75])\n    axes[i].set_ylabel(\"Occupancy (%)\")\n    axes[i].set_xlabel(\"Period\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.12: Running wheel occupancy during light and dark periods\n\n\n\n\n\nAs we would expect, both mice tend to spend more time on the running wheel during the dark (active) periods.\nEven though the mouse in kage17 is more active overall, as we saw in Section 5.6, it spends less time on the running wheel compared to the mouse in kage14.\n\n\n\n\nDunn, Timothy W., Jesse D. Marshall, Kyle S. Severson, Diego E. Aldarondo, David G. C. Hildebrand, Selmaan N. Chettih, William L. Wang, et al. 2021. “Geometric Deep Learning Enables 3D Kinematic Profiling Across Species and Environments.” Nature Methods 18 (5): 564–73. https://doi.org/10.1038/s41592-021-01106-6.\n\n\nHo, Hinze, Nejc Kejzar, Hiroki Sasaguri, Takashi Saito, Takaomi C. Saido, Bart De Strooper, Marius Bauza, and Julija Krupic. 2023. “A Fully Automated Home Cage for Long-Term Continuous Phenotyping of Mouse Cognition and Behavior.” Cell Reports Methods 3 (7): 100532. https://doi.org/10.1016/j.crmeth.2023.100532.\n\n\nKarashchuk, Pierre, Katie L. Rupp, Evyn S. Dickinson, Sarah Walling-Bell, Elischa Sanders, Eiman Azim, Bingni W. Brunton, and John C. Tuthill. 2021. “Anipose: A Toolkit for Robust Markerless 3D Pose Estimation.” bioRxiv. https://doi.org/10.1101/2020.05.26.117325.\n\n\nPereira, Talmo D., Joshua W. Shaevitz, and Mala Murthy. 2020. “Quantifying Behavior to Understand the Brain.” Nature Neuroscience 23 (12): 1537–49. https://doi.org/10.1038/s41593-020-00734-z.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html",
    "href": "06-movement-zebras.html",
    "title": "6  Zebra escape trajectories",
    "section": "",
    "text": "Before we start, make sure you have created the animals-in-motion-env environment (see prerequisites A.3.3), and are using it to run this notebook.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "Appendix A — Prerequisites",
    "section": "",
    "text": "A.1 Knowledge\nWe assume basic familiarity with Python, ideally including its core scientific libraries such as NumPy, Pandas, Matplotlib, and Jupyter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#hardware",
    "href": "prerequisites.html#hardware",
    "title": "Appendix A — Prerequisites",
    "section": "A.2 Hardware",
    "text": "A.2 Hardware\nThis is a hands-on course, so please bring your own laptop and charger.\nA mouse is strongly recommended, especially for tasks like image annotation.\nA dedicated GPU is not required, though it may speed up some computations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#software",
    "href": "prerequisites.html#software",
    "title": "Appendix A — Prerequisites",
    "section": "A.3 Software",
    "text": "A.3 Software\nYou’ll need both general tools for Python programming and specific software required for the course, as detailed below.\n\nA.3.1 General development tools\n\n\n\n\n\n\nNote\n\n\n\nIf you already have a working Anaconda or Miniconda installation and have used it to run Python scripts or Jupyter notebooks, you can likely skip the steps below.\n\n\nTo prepare your computer for Python development, we recommend following the Software Carpentries installation instructions, in particular:\n\nBash Shell, to run terminal commands\nGit, including a GitHub account\nPython, via the conda-forge installer. Please make sure you install a Python version &gt;= 3.12 (e.g. 3.12 is fine, 3.10 is not).\n\nYou’ll also need a code editor (IDE) configured for Python.\nIf you already have one you’re comfortable with, feel free to use it. Otherwise, we recommend:\n\nVisual Studio Code with the Python extension\nJupyterLab\n\n\n\nA.3.2 For the SLEAP tutorial\nPlease install SLEAP following the official installation instructions.\n\n\n\n\n\n\nNote\n\n\n\nFor this workshop, use SLEAP version 1.3.4. Be sure to replace the default version number (e.g. 1.4.1) in the instructions with 1.3.4.\n\n\nThis should create a conda environment named sleap with the necessary dependencies. You can verify the installation by running:\nconda activate sleap\nsleap-label\nThis should launch the SLEAP graphical user interface (GUI).\n\n\nA.3.3 For the interactive notebooks\nYou will also need a separate conda environment with everything required for the interactive exercises, including the movement and jupyter packages.\nWe recommend cloning this workshop’s repository and creating the environment using the provided environment.yaml file:\ngit clone https://github.com/neuroinformatics-unit/animals-in-motion.git\ncd animals-in-motion\nconda env create -n animals-in-motion-env -f environment.yaml\nTo test your setup, run:\nconda activate animals-in-motion-env\nmovement launch\nThis should open the movement GUI, i.e. the napari image viewer with the movement plugin docked on the right.\n\n\n\n\n\n\nNote\n\n\n\nThere are other ways to install the movement package.\nHowever, for this workshop, we recommend using the environment.yaml file to ensure that all necessary dependencies, including those beyond movement, are included.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#sec-data",
    "href": "prerequisites.html#sec-data",
    "title": "Appendix A — Prerequisites",
    "section": "A.4 Data",
    "text": "A.4 Data\nBringing your own data is encouraged but not required. This could include video recordings of animal behaviour and/or motion tracking data you’ve previously generated.\nWe also provide some example datasets for you to use during the workshop. Please download these from Dropbox before the workshop starts (they are a few GB in size).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Appendix B — Contributing",
    "section": "",
    "text": "B.1 Setting up the development environment\nThank you for considering contributing to the Animals In Motion project! We welcome contributions in various forms, including bug reports, requests for content improvement, as well as new tutorials or case studies.\nBegin by cloning the repository and navigating to its root directory:\nWe use conda to manage dependencies. First, create a development environment using the environment-dev.yaml file, and activate it:\nTo enable the pre-commit hooks, run the following command once:\nThis is a Quarto book project, with its source code located in the book/ directory. We refer you to the Quarto documentation for more information on how books are structured and configured.\nTo render/preview the book locally, you’ll need the Quarto CLI installed, as well as the VSCode Quarto extension\nYou will also need to make sure that the QUARTO_PYTHON environment variable is set to the path of the python executable in the development conda environment. This guarantees that the Quarto CLI will use the correct Python interpreter when rendering the book.\nThen, you can render the book using:\nYou can view the rendered book by opening the book/_book/index.html file in your browser.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#setting-up-the-development-environment",
    "href": "contributing.html#setting-up-the-development-environment",
    "title": "Appendix B — Contributing",
    "section": "",
    "text": "git clone https://github.com/neuroinformatics-unit/animals-in-motion.git\ncd animals-in-motion\n\nconda env create -n animals-in-motion-dev -f environment-dev.yaml\nconda activate animals-in-motion-dev\n\npre-commit install\n\n\n\nexport QUARTO_PYTHON=$(which python)\n\nquarto render book\n# or if you want to run executable code blocks before rendering to html\nquarto render book --execute",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#authoring-content",
    "href": "contributing.html#authoring-content",
    "title": "Appendix B — Contributing",
    "section": "B.2 Authoring content",
    "text": "B.2 Authoring content\nBook chapters are written primarily as Quarto Markdown files (.qmd). These can contain a mix of narrative and interactive content, such as code exercises. See Quarto computations &gt; Using Python to learn more about executable code blocks.\nWe recommend using the Quarto VSCode extension for authoring and previewing content.\nAlternatively, you may also use JupyterLab, with Jupyter Notebooks (.ipynb) as source files—see Quarto tools &gt; JupyterLab for more information.\nThe chapter source files reside in the book/ directory and have to be linked in the book/_quarto.yml file for them to show up. See Book Crossrefs on how to reference other chapters.\nBibliographical references should be added to the book/references.bib file in BibTeX format. See Quarto authoring &gt; Citations for more information.\nIn general, cross-referencing objects (e.g. figures, tables, chapters, equations, citations, etc.) should be done using the @ref syntax, e.g. See @fig-overview for more details.\n\nB.2.1 Adding answers to exercises\nThis book is configured to be rendered with or without answers to exercises, using Quarto profiles.\n\nThe _quarto.yml file defines the “default” profile for the book, which does not show the answers to exercises.\nThe _quarto-answers.yml file defines the “answers” profile, which is identical to the “default” profile, but also includes solutions to code exercises.\n\nTo add answers to code exercises, please enclose them in a block of the following form:\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\nWrite your solution here.\n\n:::\n\n:::\nThen you can control whether the answers are shown or not by passing the appropriate Quarto profile to the quarto render command:\nquarto render book --execute --profile default  # equivalent to no profile\nquarto render book --execute --profile answers\nYou can achieve the same effect by setting the QUARTO_PROFILE environment variable before rendering the book:\nexport QUARTO_PROFILE=answers\nquarto render book --execute\nIn general, it’s most convenient to show the answers while you are developing the content, and then hide them to preview the book as a student would see it.\nSee the Section B.4 for more information on how to create releases with or without answers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#pre-commit-hooks",
    "href": "contributing.html#pre-commit-hooks",
    "title": "Appendix B — Contributing",
    "section": "B.3 Pre-commit hooks",
    "text": "B.3 Pre-commit hooks\nWe use pre-commit to run checks on the codebase before committing.\nCurrent hooks include:\n\ncodespell for catching common spelling mistakes.\nmarkdownlint for (Quarto) Markdown linting and formatting.\nruff for code linting and formatting.\n\nThese will prevent code from being committed if any of these hooks fail. To run all the hooks before committing:\npre-commit run  # for staged files\npre-commit run -a  # for all files in the repository",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#sec-versioning",
    "href": "contributing.html#sec-versioning",
    "title": "Appendix B — Contributing",
    "section": "B.4 Versioning and releasing",
    "text": "B.4 Versioning and releasing\nWe use Calendar Versioning (CalVer) and specifically the YYYY.0M scheme (e.g. 2025.08 for August 2025).\nTo create a new release, first update the book/index.qmd file. Specifically, add two rows like the following to the “Versions” table:\n| `2025.08` | version used for the inaugural workshop in August 2025 |\n| `v2025.08-answers` | same as `v2025.08` but with answers to exercises |\nYou also need to create a new tag in the vYYYY.0M format (e.g. v2025.08) and push it to the repository. Don’t forget the v prefix for the tag name!\nFor example:\ngit tag v2025.08\ngit push origin --tags",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#continuous-integration-ci",
    "href": "contributing.html#continuous-integration-ci",
    "title": "Appendix B — Contributing",
    "section": "B.5 Continuous integration (CI)",
    "text": "B.5 Continuous integration (CI)\nThe CI workflow is defined in the .github/workflows/build_and_deploy.yaml file and can be triggered by:\n\nPushes to the main branch\nPull requests\nReleases, i.e. tags starting with v (e.g., v2025.08)\nManual dispatches\n\nThe workflow is built using GitHub actions and includes three jobs:\n\nlinting: running the pre-commit hooks;\nbuild: rendering the Quarto book with and without answers, and uploading the rendered artifacts;\ndeploy: deploying the book artifact(s) to the gh-pages branch (only for pushes to the main branch and releases).\n\nEach release version is deployed to a folder in the gh-pages branch, with the same name as the release tag (e.g., v2025.08). This is accompanied by a vYYYY.0M-answers folder containing a version of the book with answers to exercises (e.g. v2025.08-answers).\nThere’s also a special folder called dev that is deployed for pushes to the main branch. This folder always includes the answers to exercises.\n\nB.5.1 Previewing the book in CI\nWe use artifact.ci to preview the book that is rendered as part of our CI workflow. This is useful to check that the book renders correctly before merging a PR. To do so:\n\nGo to the “Checks” tab in the GitHub PR.\nClick on the “Build and Deploy Quarto Book” section on the left.\nIf the “Build Quarto book” action is successful, a summary section will appear under the block diagram with a link to preview the built documentation.\nClick on the link and wait for the files to be uploaded (it may take a while the first time). You may be asked to sign in to GitHub.\nOnce the upload is complete, look for book/_book-answers/index.html under the “Detected Entrypoints” section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix C — References",
    "section": "",
    "text": "Dunn, Timothy W., Jesse D. Marshall, Kyle S. Severson, Diego E.\nAldarondo, David G. C. Hildebrand, Selmaan N. Chettih, William L. Wang,\net al. 2021. “Geometric Deep Learning Enables 3D\nKinematic Profiling Across Species and Environments.” Nature\nMethods 18 (5): 564–73. https://doi.org/10.1038/s41592-021-01106-6.\n\n\nHo, Hinze, Nejc Kejzar, Hiroki Sasaguri, Takashi Saito, Takaomi C.\nSaido, Bart De Strooper, Marius Bauza, and Julija Krupic. 2023. “A\nFully Automated Home Cage for Long-Term Continuous Phenotyping of Mouse\nCognition and Behavior.” Cell Reports Methods 3 (7):\n100532. https://doi.org/10.1016/j.crmeth.2023.100532.\n\n\nKarashchuk, Pierre, Katie L. Rupp, Evyn S. Dickinson, Sarah\nWalling-Bell, Elischa Sanders, Eiman Azim, Bingni W. Brunton, and John\nC. Tuthill. 2021. “Anipose: A Toolkit for Robust Markerless\n3D Pose Estimation.” bioRxiv. https://doi.org/10.1101/2020.05.26.117325.\n\n\nLuxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric\nYttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023.\n“Open-Source Tools for Behavioral Video Analysis:\nSetup, Methods, and Best Practices.” Edited by\nDenise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh\nN. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018.\n“DeepLabCut: Markerless Pose Estimation of\nUser-Defined Body Parts with Deep Learning.” Nature\nNeuroscience 21 (9): 1281–89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nPereira, Talmo D., Joshua W. Shaevitz, and Mala Murthy. 2020.\n“Quantifying Behavior to Understand the Brain.” Nature\nNeuroscience 23 (12): 1537–49. https://doi.org/10.1038/s41593-020-00734-z.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner,\nJunyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022.\n“SLEAP: A Deep Learning System for\nMulti-Animal Pose Tracking.” Nature Methods 19 (4):\n486–95. https://doi.org/10.1038/s41592-022-01426-1.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>References</span>"
    ]
  }
]