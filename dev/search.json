[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Animals In Motion",
    "section": "",
    "text": "Preface\nThis website hosts the course materials for the Animals in Motion workshop, part of the inauguaral Neuroinformatics Unit Open Software Week.\nWhether you are attending the workshop in person, or following along the materials at your own pace, be sure to check out Appendix A — Prerequisites.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Animals In Motion",
    "section": "",
    "text": "Target audience\n\n\n\nThis course is designed for researchers and students interested in learning about free open-source tools for tracking animal motion from video footage and extracting quantitative descriptions of behaviour from motion tracks.\n\n\n\n\nWorkshop schedule\n\n\nTime\nTopics\n\n\n\n\nDay 1: morning\n1  Introduction & 2  Neural Networks Primer\n\n\nDay 1: afternoon\n3  Pose estimation with SLEAP\n\n\nDay 2: morning\n4  Analysing tracks with movement\n\n\nDay 2: afternoon\n5  A mouse’s daily activity log & 6  Zebra escape trajectories",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#versions",
    "href": "index.html#versions",
    "title": "Animals In Motion",
    "section": "Versions",
    "text": "Versions\nThis website is available in multiple versions:\n\nlatest: latest release version\ndev: development version, corresponding to the main branch\nv2025.08a0: alpha version for the 2025.08 release",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#funding-acknowledgements",
    "href": "index.html#funding-acknowledgements",
    "title": "Animals In Motion",
    "section": "Funding & acknowledgements",
    "text": "Funding & acknowledgements\nThe first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK. We thank the Sainsbury Wellcome Centre and the Gatsby Computational Neuroscience Unit for providing facilities for the event.\n\n\n\nLogos of workshop sponsors",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Machine learning methods for motion tracking have transformed a wide range of scientific disciplines—from neuroscience and biomechanics to conservation and ethology. Tools such as DeepLabCut (Mathis et al. 2018) and SLEAP (Pereira et al. 2022) enable researchers to track animal movements in video recordings with impressive accuracy, without the need for physical markers.\nHowever, the variety of available tools can be overwhelming (Luxem et al. 2023). It’s often unclear which tool is best suited to a given application, or how to get started. We’ll provide an overview of the approaches used for quantifying animal behaviour, and we’ll narrow down into Computer Vision (CV) methods for detecting and tracking animals in videos.\n\n\n\n\nLuxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric Yttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023. “Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and Best Practices.” Edited by Denise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018. “DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.” Nature Neuroscience 21 (9): 1281–89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner, Junyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022. “SLEAP: A Deep Learning System for Multi-Animal Pose Tracking.” Nature Methods 19 (4): 486–95. https://doi.org/10.1038/s41592-022-01426-1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-neural-networks.html",
    "href": "02-neural-networks.html",
    "title": "2  Neural Networks Primer",
    "section": "",
    "text": "All you need to know about neural networks to understand the rest of the course.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Neural Networks Primer</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html",
    "href": "03-sleap-tutorial.html",
    "title": "3  Pose estimation with SLEAP",
    "section": "",
    "text": "Before we proceed, make sure you have installed SLEAP and activated the corresponding conda environment (see prerequisites A.3.2).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html",
    "href": "04-movement-intro.html",
    "title": "4  Analysing tracks with movement",
    "section": "",
    "text": "In this tutorial, we will first introduce the movement package by walking through parts of its documentation.\nAfter that you will be given a set of tasks to complete—using movement to analyse the pose tracks you’ve generated in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html",
    "href": "05-movement-mouse.html",
    "title": "5  A mouse’s daily activity log",
    "section": "",
    "text": "Before we start, make sure you have created the animals-in-motion-env environment (see prerequisites A.3.3), and are using it to run this notebook.\nLet’s download some sample data from the movement package.\n\nfrom movement import sample_data\n\nDownloading data from 'https://gin.g-node.org/neuroinformatics/movement-test-data/raw/master/metadata.yaml' to file '/Users/nsirmpilatze/.movement/data/temp_metadata.yaml'.\nSHA256 hash of downloaded file: cf2876bab4f754d48d3c9f113ce5ac91787304cc587d33d8bf1124d5358e957f\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n\n\nWe will use the following 3 files:\n\nDLC_smart-kage3_datetime-20240417T090006.predictions.h5\nDLC_smart-kage3_datetime-20240417T100006.predictions.h5\nDLC_smart-kage3_datetime-20240417T110006.predictions.h5\n\nEach file corresponds to a 1-hour segment of a video recording of a mouse in its home cage. The start date and time of each file is indicated in the filename, in the format YYYYMMDDTHHMMSS.\n\nfilename_list = [\n    \"DLC_smart-kage3_datetime-20240417T090006.predictions.h5\",\n    \"DLC_smart-kage3_datetime-20240417T100006.predictions.h5\",\n    \"DLC_smart-kage3_datetime-20240417T110006.predictions.h5\",\n]\n\nds_list = []  # list of loaded datasets\nfor filename in filename_list:\n    ds = sample_data.fetch_dataset(filename, with_video=True)\n    ds_list.append(ds)\n\nds_list[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:      (time: 7161, space: 2, keypoints: 8, individuals: 1)\nCoordinates: (4)\nData variables:\n    position     (time, space, keypoints, individuals) float64 917kB 95.43 .....\n    confidence   (time, keypoints, individuals) float64 458kB 0.2016 ... 0.00...\nAttributes: (7)xarray.DatasetDimensions:time: 7161space: 2keypoints: 8individuals: 1Coordinates: (4)time(time)float640.0 0.5 1.0 ... 3.58e+03 3.58e+03array([0.0000e+00, 5.0000e-01, 1.0000e+00, ..., 3.5790e+03, 3.5795e+03,\n       3.5800e+03], shape=(7161,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U10'snout' 'leftear' ... 'tailbase'array(['snout', 'leftear', 'rightear', 'neck', 'spine1', 'bodycenter',\n       'spine2', 'tailbase'], dtype='&lt;U10')individuals(individuals)&lt;U12'individual_0'array(['individual_0'], dtype='&lt;U12')Data variables: (2)position(time, space, keypoints, individuals)float6495.43 94.91 93.82 ... 154.8 135.7array([[[[ 95.43035126],\n         [ 94.91295624],\n         [ 93.82420349],\n         ...,\n         [ 94.53997803],\n         [ 97.54236603],\n         [100.02811432]],\n\n        [[142.89202881],\n         [142.62258911],\n         [142.43545532],\n         ...,\n         [150.54605103],\n         [150.50001526],\n         [136.07546997]]],\n\n\n       [[[ 94.78144073],\n         [ 96.83626556],\n         [ 95.19823456],\n...\n         [155.3740387 ],\n         [152.75170898],\n         [  3.44333982]]],\n\n\n       [[[100.22026062],\n         [102.41629028],\n         [100.72286224],\n         ...,\n         [103.19419098],\n         [104.8234024 ],\n         [104.95584869]],\n\n        [[146.44120789],\n         [153.16015625],\n         [148.50932312],\n         ...,\n         [154.77001953],\n         [154.75556946],\n         [135.68525696]]]], shape=(7161, 2, 8, 1))confidence(time, keypoints, individuals)float640.2016 0.9843 ... 0.938 0.001413array([[[0.2015924 ],\n        [0.98432779],\n        [0.60495883],\n        ...,\n        [0.99861991],\n        [0.98142618],\n        [0.00468036]],\n\n       [[0.20323476],\n        [0.99107742],\n        [0.71139169],\n        ...,\n        [0.99839431],\n        [0.98050493],\n        [0.00163546]],\n\n       [[0.25993589],\n        [0.98959559],\n        [0.66880894],\n        ...,\n...\n        ...,\n        [0.998694  ],\n        [0.94877648],\n        [0.00180058]],\n\n       [[0.15757136],\n        [0.77394593],\n        [0.63837171],\n        ...,\n        [0.99911445],\n        [0.96076256],\n        [0.00157974]],\n\n       [[0.1380917 ],\n        [0.90967095],\n        [0.62035251],\n        ...,\n        [0.9988513 ],\n        [0.93795907],\n        [0.00141331]]], shape=(7161, 8, 1))Indexes: (4)timePandasIndexPandasIndex(Index([   0.0,    0.5,    1.0,    1.5,    2.0,    2.5,    3.0,    3.5,    4.0,\n          4.5,\n       ...\n       3575.5, 3576.0, 3576.5, 3577.0, 3577.5, 3578.0, 3578.5, 3579.0, 3579.5,\n       3580.0],\n      dtype='float64', name='time', length=7161))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['snout', 'leftear', 'rightear', 'neck', 'spine1', 'bodycenter',\n       'spine2', 'tailbase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['individual_0'], dtype='object', name='individuals'))Attributes: (7)source_software :DeepLabCutds_type :posesfps :2.0time_unit :secondssource_file :/Users/nsirmpilatze/.movement/data/poses/DLC_smart-kage3_datetime-20240417T090006.predictions.h5frame_path :/Users/nsirmpilatze/.movement/data/frames/smart-kage3_datetime-20240417T090006_frame-0.pngvideo_path :/Users/nsirmpilatze/.movement/data/videos/smart-kage3_datetime-20240417T090006_video.mp4",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html",
    "href": "06-movement-zebras.html",
    "title": "6  Zebra escape trajectories",
    "section": "",
    "text": "Before we start, make sure you have created the animals-in-motion-env environment (see prerequisites A.3.3), and are using it to run this notebook.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "Appendix A — Prerequisites",
    "section": "",
    "text": "A.1 Knowledge\nWe assume basic familiarity with Python, ideally including its core scientific libraries such as NumPy, Pandas, Matplotlib, and Jupyter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#hardware",
    "href": "prerequisites.html#hardware",
    "title": "Appendix A — Prerequisites",
    "section": "A.2 Hardware",
    "text": "A.2 Hardware\nThis is a hands-on course, so please bring your own laptop and charger.\nA mouse is strongly recommended, especially for tasks like image annotation.\nA dedicated GPU is not required, though it may speed up some computations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#software",
    "href": "prerequisites.html#software",
    "title": "Appendix A — Prerequisites",
    "section": "A.3 Software",
    "text": "A.3 Software\nYou’ll need both general tools for Python programming and specific software required for the course, as detailed below.\n\nA.3.1 General development tools\n\n\n\n\n\n\nNote\n\n\n\nIf you already have a working Anaconda or Miniconda installation and have used it to run Python scripts or Jupyter notebooks, you can likely skip the steps below.\n\n\nTo prepare your computer for Python development, we recommend following the Software Carpentries installation instructions, in particular:\n\nBash Shell, to run terminal commands\nGit, including a GitHub account\nPython, via the conda-forge installer\n\nYou’ll also need a code editor (IDE) configured for Python.\nIf you already have one you’re comfortable with, feel free to use it. Otherwise, we recommend:\n\nVisual Studio Code with the Python extension\nJupyterLab\n\n\n\nA.3.2 For the SLEAP tutorial\nPlease install SLEAP following the official instructions here.\n\n\n\n\n\n\nNote\n\n\n\nFor this workshop, use SLEAP version 1.3.3. Be sure to replace the default version number (e.g. 1.4.1) in the instructions with 1.3.3.\n\n\nThis should create a conda environment named sleap with the necessary dependencies. You can verify the installation by running:\nconda activate sleap\nsleap-label\nThis should launch the SLEAP graphical user interface (GUI).\n\n\nA.3.3 For the interactive notebooks\nYou will also need a separate conda environment with everything required for the interactive exercises, including the movement and jupyter packages.\nWe recommend cloning this workshop’s repository and creating the environment using the provided environment.yaml file:\ngit clone https://github.com/neuroinformatics-unit/animals-in-motion.git\ncd animals-in-motion\nconda env create -n animals-in-motion-env -f environment.yaml\nTo test your setup, run:\nconda activate animals-in-motion-env\nmovement launch\nThis should open the movement GUI, i.e. the napari image viewer with the movement plugin docked on the right.\n\n\n\n\n\n\nNote\n\n\n\nYou can find full installation instructions for movement here.\nHowever, For this workshop, we recommend using the environment.yaml file to ensure that all necessary dependencies, including those beyond movement, are included.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#data",
    "href": "prerequisites.html#data",
    "title": "Appendix A — Prerequisites",
    "section": "A.4 Data",
    "text": "A.4 Data\nBringing your own data is encouraged but not required.\nThis could include video recordings of animal behaviour and/or motion tracking data you’ve previously generated.\nIf you don’t have your own data, we’ll provide example datasets for you to use during the workshop.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Appendix B — Contributing",
    "section": "",
    "text": "B.1 Setting up the development environment\nThank you for considering contributing to the Animals In Motion project! We welcome contributions in various forms, including bug reports, feature requests, and code contributions.\nBegin by cloning the repository and navigating to its root directory:\nWe use conda to manage dependencies. First, create a development environment using the environment-dev.yaml file, and activate it:\nTo enable the pre-commit hooks, run the following command once:\nThis is a Quarto book project, with its source code located in the book/ directory. We refer you to the Quarto documentation books are structured.\nTo render/preview the book locally, you’ll need the Quarto CLI installed, as well as the VSCode Quarto extension\nYou will also need to make sure that the QUARTO_PYTHON environment variable is set to the path of the python executable in the development conda environment. This guarantees that the Quarto CLI will use the correct Python interpreter when rendering the book.\nThen, you can render the book using:\nYou can view the rendered book by opening the book/_book/index.html file in your browser.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#setting-up-the-development-environment",
    "href": "contributing.html#setting-up-the-development-environment",
    "title": "Appendix B — Contributing",
    "section": "",
    "text": "git clone https://github.com/neuroinformatics-unit/animals-in-motion.git\ncd animals-in-motion\n\nconda env create -n animals-in-motion-dev -f environment-dev.yaml\nconda activate animals-in-motion-dev\n\npre-commit install\n\n\n\nexport QUARTO_PYTHON=$(which python)\n\nquarto render book\n# or if you want to run executable code blocks before rendering to html\nquarto render book --execute",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#pre-commit-hooks",
    "href": "contributing.html#pre-commit-hooks",
    "title": "Appendix B — Contributing",
    "section": "B.2 Pre-commit hooks",
    "text": "B.2 Pre-commit hooks\nWe use pre-commit to run checks on the codebase before committing.\nCurrent hooks include: - codespell for catching common spelling mistakes - ruff for code linting and formatting\nThese will prevent code from being committed if any of these hooks fail. To run all the hooks before committing:\npre-commit run  # for staged files\npre-commit run -a  # for all files in the repository",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#versioning-and-releasing",
    "href": "contributing.html#versioning-and-releasing",
    "title": "Appendix B — Contributing",
    "section": "B.3 Versioning and releasing",
    "text": "B.3 Versioning and releasing\nWe use Calendar Versioning (CalVer) and specifically the YYYY.0M scheme (e.g. 2025.08 for August 2025).\nTo create a new release, first update the book/index.qmd file. Specifically, add an entry like the following under the “Versions” section:\n- [v2025.08](https://animals-in-motion.neuroinformatics.dev/v2025.08/): Version used for the inaugural workshop in August 2025\nYou also need to create a new tag with the format vYYYY.0M, and push it to the repository. Don’t forget the v prefix for the tag name!\nFor example:\ngit tag v2025.08\ngit push origin --tags",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#continuous-integration-ci",
    "href": "contributing.html#continuous-integration-ci",
    "title": "Appendix B — Contributing",
    "section": "B.4 Continuous integration (CI)",
    "text": "B.4 Continuous integration (CI)\nThe CI workflow is defined in the build_and_deploy.yaml file and can be triggered by:\n\nPushes to the main branch\nPull requests\nReleases, i.e. tags starting with v (e.g., v2025.08)\nManual dispatches\n\nThe workflow is built using GitHub actions and includes three jobs:\n\nlinting: running the pre-commit hooks\nbuild: rendering the Quarto book and uploading an artifact\ndeploy: deploying the book artifact to the gh-pages branch (only for pushes to the main branch and releases)\n\nEach release version is deployed to a folder in the gh-pages branch, with the same name as the release tag (e.g., v2025.08/). There’s also a special folder called dev/ that is deployed for pushes to the main branch.\nThe contents of the latest release are also copied to the latest/ folder, where the home page is redirected to.\nLinks to previous versions can be added to the book’s index.qmd file, under the “View other versions” section. Note that these links will only work on the deployed version of the book, not on the local version.\n\nB.4.1 Previewing the book in CI\nWe use artifact.ci to preview the book that is rendered as part of our CI workflow. This is useful to check that the book renders correctly before merging a PR. To do so:\n\nGo to the “Checks” tab in the GitHub PR.\nClick on the “Build and Deploy Quarto Book” section on the left.\nIf the “Build Quarto book” action is successful, a summary section will appear under the block diagram with a link to preview the built documentation.\nClick on the link and wait for the files to be uploaded (it may take a while the first time). You may be asked to sign in to GitHub.\nOnce the upload is complete, look for book/_book/index.html under the “Detected Entrypoints” section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix C — References",
    "section": "",
    "text": "Luxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric\nYttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023.\n“Open-Source Tools for Behavioral Video Analysis:\nSetup, Methods, and Best Practices.” Edited by\nDenise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh\nN. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018.\n“DeepLabCut: Markerless Pose Estimation of\nUser-Defined Body Parts with Deep Learning.” Nature\nNeuroscience 21 (9): 1281–89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner,\nJunyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022.\n“SLEAP: A Deep Learning System for\nMulti-Animal Pose Tracking.” Nature Methods 19 (4):\n486–95. https://doi.org/10.1038/s41592-022-01426-1.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>References</span>"
    ]
  }
]