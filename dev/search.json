[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Animals In Motion",
    "section": "",
    "text": "Preface\nThis website hosts the course materials for the Animals in Motion workshop, part of the inauguaral Neuroinformatics Unit Open Software Week.\nWhether you are attending the workshop in person, or following along the materials at your own pace, be sure to check out Appendix A ‚Äî Prerequisites.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Animals In Motion",
    "section": "",
    "text": "Target audience\n\n\n\nThis course is designed for researchers and students interested in learning about free open-source tools for tracking animal motion from video footage and extracting quantitative descriptions of behaviour from motion tracks.\n\n\n\n\nWorkshop schedule\n\n\nTime\nTopics\n\n\n\n\nDay 1: morning\n1¬† Introduction & 2¬† Neural Networks Primer\n\n\nDay 1: afternoon\n3¬† Pose estimation with SLEAP\n\n\nDay 2: morning\n4¬† A mouse‚Äôs daily activity log\n\n\nDay 2: afternoon\n5¬† Zebra escape trajectories",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#versions",
    "href": "index.html#versions",
    "title": "Animals In Motion",
    "section": "Versions",
    "text": "Versions\nThis website is available in multiple versions:\n\nlatest: latest release version\ndev: development version, corresponding to the main branch\nv2025.08a0: alpha version for the 2025.08 release",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#funding-acknowledgements",
    "href": "index.html#funding-acknowledgements",
    "title": "Animals In Motion",
    "section": "Funding & acknowledgements",
    "text": "Funding & acknowledgements\nThe first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK. We thank the Sainsbury Wellcome Centre and the Gatsby Computational Neuroscience Unit for providing facilities for the event.\n\n\n\nLogos of workshop sponsors",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "Machine learning methods for motion tracking have transformed a wide range of scientific disciplines‚Äîfrom neuroscience and biomechanics to conservation and ethology. Tools such as DeepLabCut (Mathis et al. 2018) and SLEAP (Pereira et al. 2022) enable researchers to track animal movements in video recordings with impressive accuracy, without the need for physical markers.\nHowever, the variety of available tools can be overwhelming (Luxem et al. 2023). It‚Äôs often unclear which tool is best suited to a given application, or how to get started. We‚Äôll provide an overview of the approaches used for quantifying animal behaviour, and we‚Äôll narrow down into Computer Vision (CV) methods for detecting and tracking animals in videos.\n\n\n\n\nLuxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric Yttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023. ‚ÄúOpen-Source Tools for Behavioral Video Analysis: Setup, Methods, and Best Practices.‚Äù Edited by Denise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018. ‚ÄúDeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.‚Äù Nature Neuroscience 21 (9): 1281‚Äì89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner, Junyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022. ‚ÄúSLEAP: A Deep Learning System for Multi-Animal Pose Tracking.‚Äù Nature Methods 19 (4): 486‚Äì95. https://doi.org/10.1038/s41592-022-01426-1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-neural-networks.html",
    "href": "02-neural-networks.html",
    "title": "2¬† Neural Networks Primer",
    "section": "",
    "text": "All you need to know about neural networks to understand the rest of the course.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Neural Networks Primer</span>"
    ]
  },
  {
    "objectID": "04-movement-mouse.html",
    "href": "04-movement-mouse.html",
    "title": "4¬† A mouse‚Äôs daily activity log",
    "section": "",
    "text": "Before we start, make sure you have the movement package installed, see prerequisites Section A.3.2.\nüì• Download this notebook\nLet‚Äôs download some sample data from the movement package.\n\nfrom movement import sample_data\n\nDownloading data from 'https://gin.g-node.org/neuroinformatics/movement-test-data/raw/master/metadata.yaml' to file '/Users/nsirmpilatze/.movement/data/temp_metadata.yaml'.\nSHA256 hash of downloaded file: cf2876bab4f754d48d3c9f113ce5ac91787304cc587d33d8bf1124d5358e957f\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n\n\nWe will use the following 3 files:\n\nDLC_smart-kage3_datetime-20240417T090006.predictions.h5\nDLC_smart-kage3_datetime-20240417T100006.predictions.h5\nDLC_smart-kage3_datetime-20240417T110006.predictions.h5\n\nEach file corresponds to a 1-hour segment of a video recording of a mouse in its home cage. The start date and time of each file is indicated in the filename, in the format YYYYMMDDTHHMMSS.\n\nfilemame_list = [\n    \"DLC_smart-kage3_datetime-20240417T090006.predictions.h5\",\n    \"DLC_smart-kage3_datetime-20240417T100006.predictions.h5\",\n    \"DLC_smart-kage3_datetime-20240417T110006.predictions.h5\",\n]\n\nds_list = []  # list of loaded datasets\nfor filename in filemame_list:\n    ds = sample_data.fetch_dataset(filename, with_video=True)\n    ds_list.append(ds)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "Appendix A ‚Äî Prerequisites",
    "section": "",
    "text": "A.1 Knowledge\nWe expect a basic familiarity with Python and ideally, with its core scientific libraries like NumPy, Pandas, Matplotlib, and Jupyter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#hardware",
    "href": "prerequisites.html#hardware",
    "title": "Appendix A ‚Äî Prerequisites",
    "section": "A.2 Hardware",
    "text": "A.2 Hardware\nThis is a hands-on course, so please bring your own laptop and charger. A mouse is also strongly recommended to facilitate tasks like image annotation. A dedicated GPU is not required, though it will speed up some of the computations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#software",
    "href": "prerequisites.html#software",
    "title": "Appendix A ‚Äî Prerequisites",
    "section": "A.3 Software",
    "text": "A.3 Software\n\nA.3.1 Coding environment\n\n\n\n\n\n\nNote\n\n\n\nIf you already have a working Anaconda/miniconda installation, and have used it to run Python scripts and/or Jupyter notebooks, you can probably skip the following steps.\n\n\nTo make your computer ready for coding in Python, we recommend following the Software Carpentries install instructions, and more specifically the following steps:\n\nBash Shell, so you can run commands in your terminal.\nGit, including a GitHub account.\nPython, which will set you up with the conda-forge installer.\n\nAdditionally, you will need a code editor (an IDE) that is configured for Python use. If you are already have one installed and configured, stick with it. Otherwise, we recommend one of the following:\n\nVisual Studio Code with the Python extension\nJupyterLab\n\n\n\nA.3.2 Specific software tools\nYou will need to install two software packages, SLEAP and movement, in separate conda environments.\nFor SLEAP, you can find the installation instructions here: https://sleap.ai/installation.html.\n\n\n\n\n\n\nNote\n\n\n\nFor this workshop, we will be using SLEAP version 1.3.3, so make sure to replace the latest version number (e.g.¬†1.4.1) with 1.3.3 in the instructions.\n\n\nThe installation instructions for movement are available here: https://movement.neuroinformatics.dev/user_guide/installation.html.\n\n\n\n\n\n\nNote\n\n\n\nAny version of movement greater than or equal to 0.8.1 will work for this workshop. You will also need to use movement‚Äôs GUI, so make sure to follow the appropriate instructions for your operating system. If you‚Äôre looking for a one-line command, the following will probably work:\nconda create -n movement-env -c conda-forge \"movement&gt;=0.8.1\" napari pyqt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#data",
    "href": "prerequisites.html#data",
    "title": "Appendix A ‚Äî Prerequisites",
    "section": "A.4 Data",
    "text": "A.4 Data\nBringing your own data is encouraged but not required. This could include video recordings of animal behaviour and/or motion tracks you‚Äôve already generated. If you don‚Äôt have your own data, we will provide example datasets for you to work with.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Appendix B ‚Äî Contributing",
    "section": "",
    "text": "B.1 Setting up the development environment\nThank you for considering contributing to the Animals In Motion project! We welcome contributions in various forms, including bug reports, feature requests, and code contributions.\nBegin by cloning the repository and navigating to its root directory:\nWe use conda to manage dependencies. First, create a development environment using the environment-dev.yaml file, and activate it:\nTo enable the pre-commit hooks, run the following command once:\nThis is a Quarto book project, with its source code located in the book/ directory. We refer you to the Quarto documentation books are structured.\nTo render/preview the book locally, you‚Äôll need the Quarto CLI installed, as well as the VSCode Quarto extension\nYou will also need to make sure that the QUARTO_PYTHON environment variable is set to the path of the python executable in the development conda environment. This guarantees that the Quarto CLI will use the correct Python interpreter when rendering the book.\nThen, you can render the book using:\nYou can view the rendered book by opening the book/_book/index.html file in your browser.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#setting-up-the-development-environment",
    "href": "contributing.html#setting-up-the-development-environment",
    "title": "Appendix B ‚Äî Contributing",
    "section": "",
    "text": "git clone https://github.com/neuroinformatics-unit/animals-in-motion.git\ncd animals-in-motion\n\nconda env create -n animals-in-motion-dev -f environment-dev.yaml\nconda activate animals-in-motion-dev\n\npre-commit install\n\n\n\nexport QUARTO_PYTHON=$(which python)\n\nquarto render book\n# or if you want to run executable code blocks before rendering to html\nquarto render book --execute",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#pre-commit-hooks",
    "href": "contributing.html#pre-commit-hooks",
    "title": "Appendix B ‚Äî Contributing",
    "section": "B.2 Pre-commit hooks",
    "text": "B.2 Pre-commit hooks\nWe use pre-commit to run checks on the codebase before committing.\nCurrent hooks include: - codespell for catching common spelling mistakes - ruff for code linting and formatting\nThese will prevent code from being committed if any of these hooks fail. To run all the hooks before committing:\npre-commit run  # for staged files\npre-commit run -a  # for all files in the repository",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#versioning-and-releasing",
    "href": "contributing.html#versioning-and-releasing",
    "title": "Appendix B ‚Äî Contributing",
    "section": "B.3 Versioning and releasing",
    "text": "B.3 Versioning and releasing\nWe use Calendar Versioning (CalVer) and specifically the YYYY.0M scheme (e.g.¬†2025.08 for August 2025).\nTo create a new release, first update the book/index.qmd file. Specifically, add an entry like the following under the ‚ÄúVersions‚Äù section:\n- [v2025.08](https://animals-in-motion.neuroinformatics.dev/v2025.08/): Version used for the inaugural workshop in August 2025\nYou also need to create a new tag with the format vYYYY.0M, and push it to the repository. Don‚Äôt forget the v prefix for the tag name!\nFor example:\ngit tag v2025.08\ngit push origin --tags",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#continuous-integration-ci",
    "href": "contributing.html#continuous-integration-ci",
    "title": "Appendix B ‚Äî Contributing",
    "section": "B.4 Continuous integration (CI)",
    "text": "B.4 Continuous integration (CI)\nThe CI workflow is defined in the build_and_deploy.yaml file and can be triggered by:\n\nPushes to the main branch\nPull requests\nReleases, i.e.¬†tags starting with v (e.g., v2025.08)\nManual dispatches\n\nThe workflow is built using GitHub actions and includes three jobs:\n\nlinting: running the pre-commit hooks\nbuild: rendering the Quarto book and uploading an artifact\ndeploy: deploying the book artifact to the gh-pages branch (only for pushes to the main branch and releases)\n\nEach release version is deployed to a folder in the gh-pages branch, with the same name as the release tag (e.g., v2025.08/). There‚Äôs also a special folder called dev/ that is deployed for pushes to the main branch.\nThe contents of the latest release are also copied to the latest/ folder, where the home page is redirected to.\nLinks to previous versions can be added to the book‚Äôs index.qmd file, under the ‚ÄúView other versions‚Äù section. Note that these links will only work on the deployed version of the book, not on the local version.\n\nB.4.1 Previewing the book in CI\nWe use artifact.ci to preview the book that is rendered as part of our CI workflow. This is useful to check that the book renders correctly before merging a PR. To do so:\n\nGo to the ‚ÄúChecks‚Äù tab in the GitHub PR.\nClick on the ‚ÄúBuild and Deploy Quarto Book‚Äù section on the left.\nIf the ‚ÄúBuild Quarto book‚Äù action is successful, a summary section will appear under the block diagram with a link to preview the built documentation.\nClick on the link and wait for the files to be uploaded (it may take a while the first time). You may be asked to sign in to GitHub.\nOnce the upload is complete, look for book/_book/index.html under the ‚ÄúDetected Entrypoints‚Äù section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix C ‚Äî References",
    "section": "",
    "text": "Luxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric\nYttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023.\n‚ÄúOpen-Source Tools for Behavioral Video Analysis:\nSetup, Methods, and Best Practices.‚Äù Edited by\nDenise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh\nN. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018.\n‚ÄúDeepLabCut: Markerless Pose Estimation of\nUser-Defined Body Parts with Deep Learning.‚Äù Nature\nNeuroscience 21 (9): 1281‚Äì89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner,\nJunyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022.\n‚ÄúSLEAP: A Deep Learning System for\nMulti-Animal Pose Tracking.‚Äù Nature Methods 19 (4):\n486‚Äì95. https://doi.org/10.1038/s41592-022-01426-1.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>References</span>"
    ]
  }
]