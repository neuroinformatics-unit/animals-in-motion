[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Animals In Motion",
    "section": "",
    "text": "Preface\nThis website hosts the course materials for the Animals in Motion workshop, part of the inauguaral Neuroinformatics Unit Open Software Week.\nWhether you are attending the workshop in person, or following along the materials at your own pace, be sure to check out Appendix A — Prerequisites.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Animals In Motion",
    "section": "",
    "text": "Target audience\n\n\n\nThis course is designed for researchers and students interested in learning about free open-source tools for tracking animal motion from video footage and extracting quantitative descriptions of behaviour from motion tracks.\n\n\n\n\nWorkshop schedule\n\n\nTime\nTopics\n\n\n\n\nDay 1: morning\n1  Introduction  2  Deep learning for computer vision primer\n\n\nDay 1: afternoon\n3  Pose estimation with SLEAP\n\n\nDay 2: morning\n4  Analysing tracks with movement\n\n\nDay 2: afternoon\n5  A mouse’s daily activity log  6  Zebra escape trajectories",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#versions",
    "href": "index.html#versions",
    "title": "Animals In Motion",
    "section": "Versions",
    "text": "Versions\nThe latest release version is always available at the following URL:\nhttps://animals-in-motion.neuroinformatics.dev/latest/\nTo view other versions, replace latest in the URL with one of the following version names:\n\n\n\nVersion\nDescription\n\n\n\n\ndev\ndevelopment version, corresponding to the main branch\n\n\nv2025.08a2\nalpha version for the 2025.08 release\n\n\n\nNon-development versions are also available with answers to exercises which can be accessed by appending -answers to the version name in the URL. For example, answers for the v2025.08a2 version are available at https://animals-in-motion.neuroinformatics.dev/v2025.08a2-answers/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#funding-acknowledgements",
    "href": "index.html#funding-acknowledgements",
    "title": "Animals In Motion",
    "section": "Funding & acknowledgements",
    "text": "Funding & acknowledgements\nThe first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK.\nWe thank the Sainsbury Wellcome Centre and the Gatsby Computational Neuroscience Unit for providing facilities for the event.\n\n\n\nLogos of workshop sponsors",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Machine learning methods for motion tracking have transformed a wide range of scientific disciplines—from neuroscience and biomechanics to conservation and ethology. Tools such as DeepLabCut (Mathis et al. 2018) and SLEAP (Pereira et al. 2022) enable researchers to track animal movements in video recordings with impressive accuracy, without the need for physical markers.\nHowever, the variety of available tools can be overwhelming (Luxem et al. 2023). It’s often unclear which tool is best suited to a given application, or how to get started. We’ll provide an overview of the approaches used for quantifying animal behaviour, and we’ll narrow down into Computer Vision (CV) methods for detecting and tracking animals in videos.\n\n\n\n\nLuxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric Yttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023. “Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and Best Practices.” Edited by Denise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018. “DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.” Nature Neuroscience 21 (9): 1281–89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner, Junyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022. “SLEAP: A Deep Learning System for Multi-Animal Pose Tracking.” Nature Methods 19 (4): 486–95. https://doi.org/10.1038/s41592-022-01426-1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-deep-learning.html",
    "href": "02-deep-learning.html",
    "title": "2  Deep learning for computer vision primer",
    "section": "",
    "text": "All you need to know about Deep learning and its applications to computer vision to understand the rest of the course.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Deep learning for computer vision primer</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html",
    "href": "03-sleap-tutorial.html",
    "title": "3  Pose estimation with SLEAP",
    "section": "",
    "text": "Before we proceed, make sure you have installed SLEAP and activated the corresponding conda environment (see prerequisites A.3.2).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html",
    "href": "04-movement-intro.html",
    "title": "4  Analysing tracks with movement",
    "section": "",
    "text": "In this tutorial, we will first introduce the movement package by walking through parts of its documentation.\nAfter that you will be given a set of tasks to complete—using movement to analyse the pose tracks you’ve generated in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html",
    "href": "05-movement-mouse.html",
    "title": "5  A mouse’s daily activity log",
    "section": "",
    "text": "5.1 Import libraries\nIn this case study, we’ll be using the movement package to dive into mouse home cage monitoring data acquired in Smart-Kages and tracked with DeepLabCut. We’ll explore how mouse activity levels fluctuate throughout the day.\nBefore you get started, make sure you’ve set up the animals-in-motion-env environment (refer to prerequisites A.3.3) and are using it to run this notebook. You’ll also need to download the Smart-Kages.zip archive from Dropbox (see prerequisites A.4) and unzip it.\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\nfrom movement import sample_data\nfrom movement.filtering import filter_by_confidence\nfrom movement.kinematics import compute_speed\nfrom movement.plots import plot_occupancy\n\nDownloading data from 'https://gin.g-node.org/neuroinformatics/movement-test-data/raw/master/metadata.yaml' to file '/home/runner/.movement/data/temp_metadata.yaml'.\nSHA256 hash of downloaded file: cf2876bab4f754d48d3c9f113ce5ac91787304cc587d33d8bf1124d5358e957f\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#the-smart-kages-dataset",
    "href": "05-movement-mouse.html#the-smart-kages-dataset",
    "title": "5  A mouse’s daily activity log",
    "section": "5.2 The Smart-Kages dataset",
    "text": "5.2 The Smart-Kages dataset\n\n\n\n\n\n\nAcknowledgement\n\n\n\nThis dataset was kindly shared by Loukia Katsouri from the O’Keefe Lab, with permission to use for this workshop.\n\n\nThe Smart-Kages dataset comprises home cage recordings from two mice, each housed in a specialised Smart-Kage (Ho et al. 2023)—a home cage monitoring system equipped with a camera mounted atop the cage.\nThe camera captures data around the clock at a rate of 2 frames per second, saving a video segment for each hour of the day. A pre-trained DeepLabCut model is subsequently employed to predict 8 keypoints on the mouse’s body.\nLet’s examine the contents of the downloaded data. You will need to specify the path to the unzipped Smart-Kages folder on your machine.\n\n# Replace with the path to the unzipped Smart-Kages folder on your machine\nsmart_kages_path = Path.home() / \".movement\" / \"Smart-Kages\"\n\n# Let's visualise the contents of the folder\nfiles = [f.name for f in smart_kages_path.iterdir()]\nfiles.sort()\nfor file in files:\n    print(file)\n\nkage14.nc\nkage14_background.png\nkage17.nc\nkage17_background.png\n\n\nThe tracking data are stored in two.nc (NetCDF) files: kage14 and kage17. NetCDF is an HDF5-based file format that can be natively saved/loaded by the xarray library, and is therefore convenient to use with movement.\nApart from these, we also have two .png files: kage14_background.png and kage17_background.png, which constitute frames extracted from the videos.\nLet’s take a look at them.\n\n\nCode\nkages = [\"kage14\", \"kage17\"]\nimg_paths = [smart_kages_path / f\"{kage}_background.png\" for kage in kages]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\nfor i, img_path in enumerate(img_paths):\n    img = plt.imread(img_path)\n    axes[i].imshow(img)\n    axes[i].set_title(f\"{kages[i]}\")\n    axes[i].axis(\"off\")\n\n\n\n\n\n\n\n\nFigure 5.1: Top-down camera views of the Smart-Kage habitats\n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhat objects do you see in the habitat?\nWhat challenges do you anticipate with tracking a mouse in this environment?\nWhat are the trade-offs one has to consider when designing a continuous monitoring system?\n\n\n\nLet’s load and inspect the tracking data:\n\nds_kages = {}  # a dictionary to store kage name -&gt; xarray dataset\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage] = xr.open_dataset(smart_kages_path / f\"{kage}.nc\")\n\nds_kages[\"kage14\"]   # Change to \"kage17\" to inspect the other dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:          (time: 5236793, space: 2, keypoints: 8, individuals: 1)\nCoordinates: (5)\nData variables:\n    position         (time, space, keypoints, individuals) float64 670MB ...\n    confidence       (time, keypoints, individuals) float64 335MB ...\nAttributes: (7)xarray.DatasetDimensions:time: 5236793space: 2keypoints: 8individuals: 1Coordinates: (5)space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U10'snout' 'leftear' ... 'tailbase'array(['snout', 'leftear', 'rightear', 'neck', 'spine1', 'bodycenter',\n       'spine2', 'tailbase'], dtype='&lt;U10')individuals(individuals)&lt;U12'individual_0'array(['individual_0'], dtype='&lt;U12')time(time)datetime64[ns]2024-04-08T13:55:40 ... 2024-05-...array(['2024-04-08T13:55:40.000000000', '2024-04-08T13:55:40.499044000',\n       '2024-04-08T13:55:40.998088000', ..., '2024-05-10T07:59:58.501344000',\n       '2024-05-10T07:59:59.001274000', '2024-05-10T07:59:59.501205000'],\n      shape=(5236793,), dtype='datetime64[ns]')seconds_elapsed(time)float64...[5236793 values with dtype=float64]Data variables: (2)position(time, space, keypoints, individuals)float64...[83788688 values with dtype=float64]confidence(time, keypoints, individuals)float64...[41894344 values with dtype=float64]Indexes: (4)spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['snout', 'leftear', 'rightear', 'neck', 'spine1', 'bodycenter',\n       'spine2', 'tailbase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['individual_0'], dtype='object', name='individuals'))timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-08 13:55:40', '2024-04-08 13:55:40.499044',\n               '2024-04-08 13:55:40.998088', '2024-04-08 13:55:41.497133',\n               '2024-04-08 13:55:41.996177', '2024-04-08 13:55:42.495221',\n               '2024-04-08 13:55:42.994266', '2024-04-08 13:55:43.493310',\n               '2024-04-08 13:55:43.992354', '2024-04-08 13:55:44.491399',\n               ...\n               '2024-05-10 07:59:55.001829', '2024-05-10 07:59:55.501760',\n               '2024-05-10 07:59:56.001691', '2024-05-10 07:59:56.501621',\n               '2024-05-10 07:59:57.001552', '2024-05-10 07:59:57.501482',\n               '2024-05-10 07:59:58.001413', '2024-05-10 07:59:58.501344',\n               '2024-05-10 07:59:59.001274', '2024-05-10 07:59:59.501205'],\n              dtype='datetime64[ns]', name='time', length=5236793, freq=None))Attributes: (7)source_software :DeepLabCutds_type :posesfps :2.0time_unit :datetime64[ns]source_file :/Users/nsirmpilatze/Data/Smart-Kages/kage14/analysis/dlc_output/2024/04/08/kage14_20240408_135536DLC_resnet101_v2Jan17shuffle2_580000.h5kage :kage14kage_start_datetime :2024-04-08T13:55:40\n\n\nWe see that each dataset contains a huge amount of data!\n\n\nCode\nstart_date_k14 = pd.to_datetime(ds_kages[\"kage14\"].time.data[0])\nend_date_k14 = pd.to_datetime(ds_kages[\"kage14\"].time.data[-1])\nduration_k14 = end_date_k14 - start_date_k14\n\nstart_date_k17 = pd.to_datetime(ds_kages[\"kage17\"].time.data[0])\nend_date_k17 = pd.to_datetime(ds_kages[\"kage17\"].time.data[-1])\nduration_k17 = end_date_k17 - start_date_k17\n\nprint(\"Experiment durations:\")\nprint(f\"kage-14: from {start_date_k14} to {end_date_k14} ({duration_k14.days} days)\")\nprint(f\"kage-17: from {start_date_k17} to {end_date_k17} ({duration_k17.days} days)\")\n\n\nExperiment durations:\nkage-14: from 2024-04-08 13:55:40 to 2024-05-10 07:59:59.501205 (31 days)\nkage-17: from 2024-04-03 00:00:06 to 2024-05-10 07:59:59.509103 (37 days)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#datetime-coordinates",
    "href": "05-movement-mouse.html#datetime-coordinates",
    "title": "5  A mouse’s daily activity log",
    "section": "5.3 Datetime Coordinates",
    "text": "5.3 Datetime Coordinates\nYou might notice something interesting about the time coordinates in these xarray datasets: they’re given in datetime64[ns] format, which means they’re precise timestamps expressed in “calendar time”.\nThis is different from what we’ve seen before in other movement datasets, where time coordinates are expressed as seconds elapsed since the start of the video, or “elapsed time”.\n\n\n\n\n\n\nHow did we get these timestamps?\n\n\n\n\n\nMany recording systems can output timestamps for each video frame. In our case, the raw data from the Smart-Kage system included the start datetime of each 1-hour-long video segment and the precise time difference between the start of each segment and every frame within it.\nUsing this information, we were able to reconstruct precise datetime coordinates for all frames throughout the entire experiment. We then concatenated the DeepLabCut predictions from all video segments and assigned the datetime coordinates to the resulting dataset. If you’re interested in the details, you can find the code in the smart-kages-movement GitHub repository.\n\n\n\nUsing “calendar time” is convenient for many applications. For example, we could cross-reference the tracking results against other data sources, such as body weight measurements. It also allows us to easily select time windows by datetime:\n\nds_14 = ds_kages[\"kage14\"]\n\n# Select a specific month\nds_14.sel(time=slice(\"2024-04-01\", \"2024-04-30\"))\n\n# Select a specific day\nds_14.sel(time=slice(\"2024-04-17 00:00:00\", \"2024-04-17 23:59:59\"))\n\n# Select a half-hour window\nds_14.sel(time=slice(\"2024-04-17 09:30:00\", \"2024-04-17 10:00:00\"))\n\nThat said, it’s still useful to also know the total time elapsed since the start of the experiment. In fact, many movement functions will expect “elapsed time” and may not work with datetime coordinates (for now).\nLuckily, it’s easy to convert datetime coordinates to “elapsed time” by simply subtracting the start datetime of the whole experiment from each timestamp.\n\n\nExpand to see how this can be done\n# Get the start datetime the experiment in kage14\nexperiment_start = ds_14.time.isel(time=0).data\n\n# Subtract the start datetime from each timestamp\ntime_elapsed = (ds_14.time.data - np.datetime64(experiment_start))\n\n# Convert to seconds\nseconds_elapsed = time_elapsed / pd.Timedelta(\"1s\")\n\n# Assign the seconds_elapsed coordinate to the \"time\" dimension\nds_14 = ds_14.assign_coords(seconds_elapsed=(\"time\", seconds_elapsed))\n\n\nWe’ve pre-computed this for convenience and stored it in a secondary time coordinate called seconds_elapsed.\n\nprint(ds_14.coords[\"time\"].values[:2])\nprint(ds_14.coords[\"seconds_elapsed\"].values[:2])\n\n['2024-04-08T13:55:40.000000000' '2024-04-08T13:55:40.499044000']\n[0.       0.499044]\n\n\nWhenever we want to switch to “elapsed time” mode, we can simply set the seconds_elapsed coordinates as the “index” of the time dimension. This means that seconds_elapsed will be used as the primary time coordinate, allowing us to select data by it.\n\nds_14.set_index(time=\"seconds_elapsed\").sel(time=slice(0, 1800))\n\n\n\n\n\n\n\nExercise A\n\n\n\nFor each of the two kages:\n\nPlot the x-axis position of the mouse’s body center over time, for the week starting on April 15th. What do you notice?\nPlot the median confidence of the body center for each day, over the entire duration of the experiment.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#filtering-out-low-confidence-predictions",
    "href": "05-movement-mouse.html#filtering-out-low-confidence-predictions",
    "title": "5  A mouse’s daily activity log",
    "section": "5.4 Filtering out low-confidence predictions",
    "text": "5.4 Filtering out low-confidence predictions\nLet’s examine the range of confidence values for each keypoint.\n\n\nCode\nkage = \"kage14\"\nconfidence = ds_kages[kage].confidence.squeeze()\n\nfig, ax = plt.subplots(figsize=(8, 3))\nconfidence.quantile(q=0.25, dim=\"time\").plot.line(\"o--\", color=\"gray\", ax=ax, label=\"25% quantile\")\nconfidence.quantile(q=0.75, dim=\"time\").plot.line(\"o--\", color=\"gray\", ax=ax, label=\"75% quantile\")\nconfidence.median(dim=\"time\").plot.line(\"o-\", color=\"black\", ax=ax, label=\"median\")\n\nax.legend()\nax.set_title(f\"{kage} confidence range\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.2: Confidence range by keypoint\n\n\n\n\n\nIt looks like the “neck”, “bodycenter”, “spine1”, and “spine2” keypoints are the most confidently detected. Let us define a list of “reliable” keypoints for later use. These are all on the mouse’s body.\n\nreliable_keypoints = [\"neck\", \"bodycenter\", \"spine1\", \"spine2\"]\n\nWe can filter out low-confidence predictions.\n\nconfidence_threshold = 0.95\n\nfor kage, ds in ds_kages.items():\n    print(f\"Filtering {kage}...\")\n    ds[\"position_filtered\"] = filter_by_confidence(\n        ds.position,\n        ds.confidence,\n        threshold=confidence_threshold,\n        print_report=True,\n    )\n    print(\"\\n\")\n\nFiltering kage14...\nNo missing points (marked as NaN) in input.\nMissing points (marked as NaN) in output:\n\nkeypoints                        snout                   leftear                 rightear                      neck                    spine1                bodycenter                    spine2                  tailbase\nindividuals                                                                                                                                                                                                                \nindividual_0  3541459/5236793 (67.63%)  2326818/5236793 (44.43%)  2670878/5236793 (51.0%)  1763878/5236793 (33.68%)  1195647/5236793 (22.83%)  1109151/5236793 (21.18%)  1762748/5236793 (33.66%)  3443803/5236793 (65.76%)\n\n\nFiltering kage17...\nNo missing points (marked as NaN) in input.\nMissing points (marked as NaN) in output:\n\nkeypoints                       snout                   leftear                  rightear                      neck                    spine1                bodycenter                    spine2                  tailbase\nindividuals                                                                                                                                                                                                                \nindividual_0  4729215/6063448 (78.0%)  4296942/6063448 (70.87%)  4380660/6063448 (72.25%)  3934047/6063448 (64.88%)  3215592/6063448 (53.03%)  1959611/6063448 (32.32%)  3568964/6063448 (58.86%)  4471744/6063448 (73.75%)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#plot-the-mouses-speed-over-time",
    "href": "05-movement-mouse.html#plot-the-mouses-speed-over-time",
    "title": "5  A mouse’s daily activity log",
    "section": "5.5 Plot the mouse’s speed over time",
    "text": "5.5 Plot the mouse’s speed over time\nLet’s define a single-point representation of the mouse’s position, which we’ll call the body_centroid. We derive this by taking the mean of the 4 reliable keypoints, using their filtered positions.\n\nfor kage, ds in ds_kages.items():\n    ds[\"body_centroid\"] = ds.position_filtered.sel(\n        individuals=\"individual_0\",  # the only individual in the dataset\n        keypoints=reliable_keypoints\n    ).mean(dim=\"keypoints\")\n\nNow let’s compute the body centroid speed as a proxy of the mouse’s speed. For compute_speed to work properly, we’ll temporarily switch to “elapsed time” mode.\n\nfor kage, ds in ds_kages.items():\n    ds[\"body_centroid_speed\"] = compute_speed(\n        ds.body_centroid.set_index(time=\"seconds_elapsed\")\n    ).assign_coords(time=ds.body_centroid.time)\n\nds_kages[\"kage14\"].body_centroid_speed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid_speed' (time: 5236793)&gt; Size: 42MB\n4.352 2.27 0.376 0.2181 0.7072 2.167 ... 0.2808 0.176 0.2587 0.2372 0.4738\nCoordinates: (2)xarray.DataArray'body_centroid_speed'time: 52367934.352 2.27 0.376 0.2181 0.7072 ... 0.2808 0.176 0.2587 0.2372 0.4738array([4.35224607, 2.26965446, 0.37596294, ..., 0.25874943, 0.23722456,\n       0.47376409], shape=(5236793,))Coordinates: (2)time(time)datetime64[ns]2024-04-08T13:55:40 ... 2024-05-...array(['2024-04-08T13:55:40.000000000', '2024-04-08T13:55:40.499044000',\n       '2024-04-08T13:55:40.998088000', ..., '2024-05-10T07:59:58.501344000',\n       '2024-05-10T07:59:59.001274000', '2024-05-10T07:59:59.501205000'],\n      shape=(5236793,), dtype='datetime64[ns]')seconds_elapsed(time)float640.0 0.499 ... 2.743e+06 2.743e+06array([0.000000e+00, 4.990440e-01, 9.980880e-01, ..., 2.743459e+06,\n       2.743459e+06, 2.743460e+06], shape=(5236793,))Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-08 13:55:40', '2024-04-08 13:55:40.499044',\n               '2024-04-08 13:55:40.998088', '2024-04-08 13:55:41.497133',\n               '2024-04-08 13:55:41.996177', '2024-04-08 13:55:42.495221',\n               '2024-04-08 13:55:42.994266', '2024-04-08 13:55:43.493310',\n               '2024-04-08 13:55:43.992354', '2024-04-08 13:55:44.491399',\n               ...\n               '2024-05-10 07:59:55.001829', '2024-05-10 07:59:55.501760',\n               '2024-05-10 07:59:56.001691', '2024-05-10 07:59:56.501621',\n               '2024-05-10 07:59:57.001552', '2024-05-10 07:59:57.501482',\n               '2024-05-10 07:59:58.001413', '2024-05-10 07:59:58.501344',\n               '2024-05-10 07:59:59.001274', '2024-05-10 07:59:59.501205'],\n              dtype='datetime64[ns]', name='time', length=5236793, freq=None))Attributes: (0)\n\n\nLet’s plot the speed over time.\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(10, 6), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds_kages[kage].body_centroid_speed.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body centroid\")\n    axes[i].set_ylabel(\"speed (pixels/sec)\")\n\nplt.show()\n\n\n\n\n\n\n\nFigure 5.3: Body centroid speed\n\n\n\n\n\n\n\n\n\n\n\nExercise B\n\n\n\n\nWhat do you notice about the overall speed fluctuations over time?\nWhat do you think is the reason for this?\nDo you think there are any differences between the two kages? Feel free to “zoom in” on specific time windows to investigate this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#plot-the-occupancy-heatmap",
    "href": "05-movement-mouse.html#plot-the-occupancy-heatmap",
    "title": "5  A mouse’s daily activity log",
    "section": "5.6 Plot the occupancy heatmap",
    "text": "5.6 Plot the occupancy heatmap\nFor the heatamp calculation to properly work, we need to temporarily set the time coordinates to “elapsed time”.\n\n\nCode\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 8), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = plt.imread(img_paths[i])\n    height, width = img.shape[:2]\n\n    axes[i].imshow(img)\n    plot_occupancy(\n        ds_kages[kage].body_centroid.set_index(time=\"seconds_elapsed\"),\n        ax=axes[i],\n        cmap=\"turbo\",\n        norm=\"log\",  # log scale the colormap\n        vmax=10**6,\n        alpha=0.6,   # some transparency\n    )\n    # invert y-axis to match the video frame\n    axes[i].set_ylim([height - 1, 0])\n    axes[i].set_xlim([0, width])\n    axes[i].set_title(f\"Body centroid occupancy (log scale)\")\n\n\n\n\n\nOccupancy heatmaps\n\n\n\n\n\n\n\n\nHo, Hinze, Nejc Kejzar, Hiroki Sasaguri, Takashi Saito, Takaomi C. Saido, Bart De Strooper, Marius Bauza, and Julija Krupic. 2023. “A Fully Automated Home Cage for Long-Term Continuous Phenotyping of Mouse Cognition and Behavior.” Cell Reports Methods 3 (7): 100532. https://doi.org/10.1016/j.crmeth.2023.100532.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html",
    "href": "06-movement-zebras.html",
    "title": "6  Zebra escape trajectories",
    "section": "",
    "text": "Before we start, make sure you have created the animals-in-motion-env environment (see prerequisites A.3.3), and are using it to run this notebook.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "Appendix A — Prerequisites",
    "section": "",
    "text": "A.1 Knowledge\nWe assume basic familiarity with Python, ideally including its core scientific libraries such as NumPy, Pandas, Matplotlib, and Jupyter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#hardware",
    "href": "prerequisites.html#hardware",
    "title": "Appendix A — Prerequisites",
    "section": "A.2 Hardware",
    "text": "A.2 Hardware\nThis is a hands-on course, so please bring your own laptop and charger.\nA mouse is strongly recommended, especially for tasks like image annotation.\nA dedicated GPU is not required, though it may speed up some computations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#software",
    "href": "prerequisites.html#software",
    "title": "Appendix A — Prerequisites",
    "section": "A.3 Software",
    "text": "A.3 Software\nYou’ll need both general tools for Python programming and specific software required for the course, as detailed below.\n\nA.3.1 General development tools\n\n\n\n\n\n\nNote\n\n\n\nIf you already have a working Anaconda or Miniconda installation and have used it to run Python scripts or Jupyter notebooks, you can likely skip the steps below.\n\n\nTo prepare your computer for Python development, we recommend following the Software Carpentries installation instructions, in particular:\n\nBash Shell, to run terminal commands\nGit, including a GitHub account\nPython, via the conda-forge installer. Please make sure you install a Python version &gt;= 3.12 (e.g. 3.12 is fine, 3.10 is not).\n\nYou’ll also need a code editor (IDE) configured for Python.\nIf you already have one you’re comfortable with, feel free to use it. Otherwise, we recommend:\n\nVisual Studio Code with the Python extension\nJupyterLab\n\n\n\nA.3.2 For the SLEAP tutorial\nPlease install SLEAP following the official installation instructions.\n\n\n\n\n\n\nNote\n\n\n\nFor this workshop, use SLEAP version 1.3.4. Be sure to replace the default version number (e.g. 1.4.1) in the instructions with 1.3.4.\n\n\nThis should create a conda environment named sleap with the necessary dependencies. You can verify the installation by running:\nconda activate sleap\nsleap-label\nThis should launch the SLEAP graphical user interface (GUI).\n\n\nA.3.3 For the interactive notebooks\nYou will also need a separate conda environment with everything required for the interactive exercises, including the movement and jupyter packages.\nWe recommend cloning this workshop’s repository and creating the environment using the provided environment.yaml file:\ngit clone https://github.com/neuroinformatics-unit/animals-in-motion.git\ncd animals-in-motion\nconda env create -n animals-in-motion-env -f environment.yaml\nTo test your setup, run:\nconda activate animals-in-motion-env\nmovement launch\nThis should open the movement GUI, i.e. the napari image viewer with the movement plugin docked on the right.\n\n\n\n\n\n\nNote\n\n\n\nThere are other ways to install the movement package.\nHowever, for this workshop, we recommend using the environment.yaml file to ensure that all necessary dependencies, including those beyond movement, are included.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#sec-data",
    "href": "prerequisites.html#sec-data",
    "title": "Appendix A — Prerequisites",
    "section": "A.4 Data",
    "text": "A.4 Data\nBringing your own data is encouraged but not required. This could include video recordings of animal behaviour and/or motion tracking data you’ve previously generated.\nWe also provide some example datasets for you to use during the workshop. Please download these from Dropbox before the workshop starts (they are a few GB in size).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Appendix B — Contributing",
    "section": "",
    "text": "B.1 Setting up the development environment\nThank you for considering contributing to the Animals In Motion project! We welcome contributions in various forms, including bug reports, requests for content improvement, as well as new tutorials or case studies.\nBegin by cloning the repository and navigating to its root directory:\nWe use conda to manage dependencies. First, create a development environment using the environment-dev.yaml file, and activate it:\nTo enable the pre-commit hooks, run the following command once:\nThis is a Quarto book project, with its source code located in the book/ directory. We refer you to the Quarto documentation for more information on how books are structured and configured.\nTo render/preview the book locally, you’ll need the Quarto CLI installed, as well as the VSCode Quarto extension\nYou will also need to make sure that the QUARTO_PYTHON environment variable is set to the path of the python executable in the development conda environment. This guarantees that the Quarto CLI will use the correct Python interpreter when rendering the book.\nThen, you can render the book using:\nYou can view the rendered book by opening the book/_book/index.html file in your browser.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#setting-up-the-development-environment",
    "href": "contributing.html#setting-up-the-development-environment",
    "title": "Appendix B — Contributing",
    "section": "",
    "text": "git clone https://github.com/neuroinformatics-unit/animals-in-motion.git\ncd animals-in-motion\n\nconda env create -n animals-in-motion-dev -f environment-dev.yaml\nconda activate animals-in-motion-dev\n\npre-commit install\n\n\n\nexport QUARTO_PYTHON=$(which python)\n\nquarto render book\n# or if you want to run executable code blocks before rendering to html\nquarto render book --execute",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#authoring-content",
    "href": "contributing.html#authoring-content",
    "title": "Appendix B — Contributing",
    "section": "B.2 Authoring content",
    "text": "B.2 Authoring content\nBook chapters are written primarily as Quarto Markdown files (.qmd). These can contain a mix of narrative and interactive content, such as code exercises. See Quarto computations &gt; Using Python to learn more about executable code blocks.\nWe recommend using the Quarto VSCode extension for authoring and previewing content.\nAlternatively, you may also use JupyterLab, with Jupyter Notebooks (.ipynb) as source files—see Quarto tools &gt; JupyterLab for more information.\nThe chapter source files reside in the book/ directory and have to be linked in the book/_quarto.yml file for them to show up. See Book Crossrefs on how to reference other chapters.\nBibliographical references should be added to the book/references.bib file in BibTeX format. See Quarto authoring &gt; Citations for more information.\nIn general, cross-referencing objects (e.g. figures, tables, chapters, equations, citations, etc.) should be done using the @ref syntax, e.g. See @fig-overview for more details.\n\nB.2.1 Adding answers to exercises\nThis book is configured to be rendered with or without answers to exercises, using Quarto profiles.\n\nThe _quarto.yml file defines the “default” profile for the book, which does not show the answers to exercises.\nThe _quarto-answers.yml file defines the “answers” profile, which is identical to the “default” profile, but also includes solutions to code exercises.\n\nTo add answers to code exercises, please enclose them in a block of the following form:\n::: {.content-visible when-profile=\"answers\"}\n\n### Solutions: Exercise A\n\nWrite your solution here.\n\n:::\nThen you can control whether the answers are shown or not by passing the appropriate Quarto profile to the quarto render command:\nquarto render book --execute --profile default  # equivalent to no profile\nquarto render book --execute --profile answers\nYou can achieve the same effect by setting the QUARTO_PROFILE environment variable before rendering the book:\nexport QUARTO_PROFILE=answers\nquarto render book --execute\nIn general, it’s most convenient to show the answers while you are developing the content, and then hide them to preview the book as a student would see it.\nSee the Section B.4 for more information on how to create releases with or without answers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#pre-commit-hooks",
    "href": "contributing.html#pre-commit-hooks",
    "title": "Appendix B — Contributing",
    "section": "B.3 Pre-commit hooks",
    "text": "B.3 Pre-commit hooks\nWe use pre-commit to run checks on the codebase before committing.\nCurrent hooks include:\n\ncodespell for catching common spelling mistakes.\nmarkdownlint for (Quarto) Markdown linting and formatting.\nruff for code linting and formatting.\n\nThese will prevent code from being committed if any of these hooks fail. To run all the hooks before committing:\npre-commit run  # for staged files\npre-commit run -a  # for all files in the repository",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#sec-versioning",
    "href": "contributing.html#sec-versioning",
    "title": "Appendix B — Contributing",
    "section": "B.4 Versioning and releasing",
    "text": "B.4 Versioning and releasing\nWe use Calendar Versioning (CalVer) and specifically the YYYY.0M scheme (e.g. 2025.08 for August 2025).\nTo create a new release, first update the book/index.qmd file. Specifically, add a row like the following to the “Versions” table:\n| `2025.08` | Version used for the inaugural workshop in August 2025 |\nYou also need to create a new tag in the vYYYY.0M format (e.g. v2025.08) and push it to the repository. Don’t forget the v prefix for the tag name!\nFor example:\ngit tag v2025.08\ngit push origin --tags",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#continuous-integration-ci",
    "href": "contributing.html#continuous-integration-ci",
    "title": "Appendix B — Contributing",
    "section": "B.5 Continuous integration (CI)",
    "text": "B.5 Continuous integration (CI)\nThe CI workflow is defined in the .github/workflows/build_and_deploy.yaml file and can be triggered by:\n\nPushes to the main branch\nPull requests\nReleases, i.e. tags starting with v (e.g., v2025.08 or v2025.08-answers)\nManual dispatches\n\nThe workflow is built using GitHub actions and includes three jobs:\n\nlinting: running the pre-commit hooks\nbuild: rendering the Quarto book with and without answers, and uploading the rendered artifacts\ndeploy: deploying the book artifact(s) to the gh-pages branch (only for pushes to the main branch and releases)\n\nEach release version is deployed to a folder in the gh-pages branch, with the same name as the release tag (e.g., v2025.08). This is accompanied by a vYYYY.0M-answers folder containing a version of the book with answers to exercises.\nThere’s also a special folder called dev that is deployed for pushes to the main branch.\n\nB.5.1 Previewing the book in CI\nWe use artifact.ci to preview the book that is rendered as part of our CI workflow. This is useful to check that the book renders correctly before merging a PR. To do so:\n\nGo to the “Checks” tab in the GitHub PR.\nClick on the “Build and Deploy Quarto Book” section on the left.\nIf the “Build Quarto book” action is successful, a summary section will appear under the block diagram with a link to preview the built documentation.\nClick on the link and wait for the files to be uploaded (it may take a while the first time). You may be asked to sign in to GitHub.\nOnce the upload is complete, look for book/_book/index.html under the “Detected Entrypoints” section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix C — References",
    "section": "",
    "text": "Ho, Hinze, Nejc Kejzar, Hiroki Sasaguri, Takashi Saito, Takaomi C.\nSaido, Bart De Strooper, Marius Bauza, and Julija Krupic. 2023. “A\nFully Automated Home Cage for Long-Term Continuous Phenotyping of Mouse\nCognition and Behavior.” Cell Reports Methods 3 (7):\n100532. https://doi.org/10.1016/j.crmeth.2023.100532.\n\n\nLuxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric\nYttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023.\n“Open-Source Tools for Behavioral Video Analysis:\nSetup, Methods, and Best Practices.” Edited by\nDenise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh\nN. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018.\n“DeepLabCut: Markerless Pose Estimation of\nUser-Defined Body Parts with Deep Learning.” Nature\nNeuroscience 21 (9): 1281–89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner,\nJunyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022.\n“SLEAP: A Deep Learning System for\nMulti-Animal Pose Tracking.” Nature Methods 19 (4):\n486–95. https://doi.org/10.1038/s41592-022-01426-1.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>References</span>"
    ]
  }
]