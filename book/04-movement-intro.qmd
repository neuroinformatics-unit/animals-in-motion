# Analysing tracks with movement {#sec-movement-intro}

In this tutorial, we will first introduce the `movement` package
by walking through parts of its [documentation](https://movement.neuroinformatics.dev/).

After that you will be given a set of tasks to complete—using `movement` to analyse the pose tracks you've generated in @sec-sleap.

```{python}
#| include: false
import xarray as xr

xr.set_options(
    display_expand_attrs=False,
    display_expand_coords=False,
    keep_attrs=True,
)
```

## What is `movement`?

In [chapters @sec-intro] and [-@sec-dl-cv] we saw how the study of animal behaviour has profited from the rise of deep learning-based markerless motion tracking tools.

In [@sec-sleap] we dove deeper into [SLEAP](https://sleap.ai/), a popular package for pose estimation and tracking.
We saw how SLEAP and similar tools—like DeepLabCut and LightningPose—detect the positions of user-defined keypoints in video frames,
group the keypoints into poses, and connect their identities across time into sequential collections we call pose tracks.

The extraction of pose tracks is often just the beginning of the analysis.
Researchers use these tracks to investigate various aspects of animal behaviour, such as kinematics, spatial navigation, social interactions, etc.
Typically, these analyses involve custom, project-specific scripts that are hard to reuse across different projects and are rarely maintained after the project's conclusion.

In response to these challenges, we saw the need for a versatile and easy-to-use toolbox that is compatible with a range of ML-based motion tracking frameworks and supports interactive data exploration and analysis.
That's where `movement` comes in.
We started building in early 2023 to answer the question: __what can I do now with these tracks?__

::: {.callout-note title="movement's mission"}

`movement`aims to facilitate the study of animal behaviour
by providing a consistent, modular interface for analysing motion tracks,
enabling steps such as data cleaning, visualisation, and motion quantification.

See [movement's mission and scope statement](https://movement.neuroinformatics.dev/community/mission-scope.html) for more details.

:::

![Overview of the `movement` package](img/movement_overview.png){#fig-movement-overview}

## A unified interface for motion tracks

`movement` aims to support all popular animal tracking frameworks and file formats,
in 2D and 3D, tracking single or multiple animals of any species.

To achieve this level of versatility, we had to identify what's common across the outputs of motion tracking tools
and how we can represent them in a standardised way.

What we came up with takes the form of collections of multi-dimensional arrays—an `xarray.Dataset` object.
Each array within a dataset is an `xarray.DataArray` object holding different aspects of the collected data (position, time, confidence scores…).
You can think of an `xarray.DataArray` as a multi-dimensional `numpy.ndarray` with pandas-style indexing and labelling.

This may sound complicated but fear not, we'll build some understanding by exploring some example datasets
that are included in the `movement` package.

```{python}
from movement import sample_data
```

### Poses datasets

First, let's see how `movement` represents pose tracks, like the ones we get with SLEAP.

![The structure of a `movement` poses dataset](img/movement_poses_dataset_card.png){#fig-movement-ds-poses}

Let's load an example dataset and explore its contents.

```{python}
poses_ds = sample_data.fetch_dataset("SLEAP_two-mice_octagon.analysis.h5")
poses_ds
```

### Bounding boxes datasets

`movement` also supports datasets that consist of bounding boxes, like the ones
we would get if we performed object detection on a video, followed by
tracking identities across time.

![The structure of a `movement` bounding boxes dataset](img/movement_bboxes_dataset_card.png){#fig-movement-ds-bboxes}

```{python}
bboxes_ds = sample_data.fetch_dataset("VIA_single-crab_MOCA-crab-1_linear-interp.csv")
bboxes_ds
```

::: {.callout-tip title="Discuss"}

- What are some limitations of `movement`'s approach? What kinds of motion tracking data cannot be accommodated?
- Can you think of alternative ways of representing these data?

See the [documentation on movement datasets](https://movement.neuroinformatics.dev/user_guide/movement_dataset.html) for more details
on `movement` data structures.
:::

### Working with xarray objects

Since `poses_ds` is an `xarray.Dataset`, we can use all of `xarray`'s intuitive interface and rich
[built-in functionalities](https://movement.neuroinformatics.dev/user_guide/movement_dataset.html#using-xarrays-built-in-functionality)
for data manipulation and analysis.

Accessing data variables and attributes (metadata) is straightforward:

```{python}
print(f"Source software: {poses_ds.source_software}")
print(f"Frames per second: {poses_ds.fps}")

poses_ds.position
```

We can select a subset of data along any dimension in a variety of ways:
by integer index (order) or coordinate label.

```{python}
# First individual, first time point
poses_ds.position.isel(individuals=0, time=0)

# 0-10 seconds, two specific keypoints
poses_ds.position.sel(time=slice(0, 10), keypoints=["EarLeft", "EarRight"])
```

We can also compute statistics across any dimension.

```{python}
# Each point's median confidence score across time
poses_ds.confidence.median(dim="time")

# Take the block mean for every 10 frames.
poses_ds.position.coarsen(time=10, boundary="trim").mean()
```

`xarray` also provides a rich set of built-in [plotting methods](https://docs.xarray.dev/en/stable/user-guide/plotting.html)
for visualising the data.

```{python}
tail_base_pos = poses_ds.sel(keypoints="TailBase").position
tail_base_pos.plot.line(x="time", row="individuals", hue="space", aspect=2, size=2.5)
```

You can also combine those with `matplotlib` figures.

```{python}
from matplotlib import pyplot as plt

colors = plt.cm.tab10.colors

fig, ax = plt.subplots()
for kp, color in zip(poses_ds.keypoints, colors):
    data = poses_ds.confidence.sel(keypoints=kp)
    data.plot.hist(
        bins=50, histtype="step", density=True, ax=ax, color=color, label=kp
    )
ax.set_ylabel("Density")
ax.set_title("Confidence histograms per keypoint")
plt.legend()
```

You may also want to export date to structures you may be more familiar with,
such as
[Pandas DataFrames](https://docs.xarray.dev/en/stable/user-guide/pandas.html)
or [NumPy arrays](https://docs.xarray.dev/en/stable/user-guide/duckarrays.html).

Export the position data array as a pandas DataFrame:

```{python}
position_df = poses_ds.position.to_dataframe(
    dim_order=["time", "individuals", "keypoints", "space"]
)
position_df.head()
```

Export data variables or coordinates as numpy arrays:

```{python}
position_array = poses_ds.position.values
print(f"Position array shape: {position_array.shape}")

time_array = poses_ds.time.values
print(f"Time array shape: {time_array.shape}")
```

For saving datasets to disk, we recommend leveraging `xarray`'s built-in
[support for the netCDF file format](https://docs.xarray.dev/en/stable/user-guide/io.html#netcdf).

```{.python}
import xarray as xr

# To save a dataset to disk
poses_ds.to_netcdf("poses_ds.nc")

# To load the dataset back from memory
poses_ds = xr.open_dataset("poses_ds.nc")
```

## Loading motion tracks

As stated above, our goal with `movement` is to enable pipelines that are input-agnostic,
meaning they are not tied to a specific motion tracking tool or data format.
Therefore, `movement`'s offers input/output functions that facilitate data flows
between various motion tracking frameworks and `movement`'s own `xarray` data structure.

Please refer to the [Input/Output section](https://movement.neuroinformatics.dev/user_guide/input_output.html)
of the `movement` documentation for more details, including a full list of
supported formats.

::: {.callout-tip title="Exercise A"}

1. Load the predictions you generated in @sec-sleap into a `movement` dataset. Feel free to use the `CalMS21/CalMS21_mouse044_task1_annotator1.slp` file that you can load from Dropbox (refer to [prerequisites @sec-data]).
2. Compute the overall minimum and maximum x,y positions.
3. Plot the centroid trajectory of a given individual across time.
4. Select a narrow time window (e.g. 10 seconds) and plot the x, y positions of a certain keypoint across time.
5. Save a narrow time window dataset to a netCDF file.

__Bonus:__ Overlay the centroid trajectory on top of the video frames (task 3) onto a frame extracted from the video.
You may find inspiration in the ["Pupil tracking" example](https://movement.neuroinformatics.dev/examples/mouse_eye_movements.html).

Useful resources:

- [Input/Output guide](https://movement.neuroinformatics.dev/user_guide/input_output.html)
- [The "Load and explore pose tracks" example](https://movement.neuroinformatics.dev/examples/load_and_explore_poses.html)
- [`plot_centroid_trajectory()` function](https://movement.neuroinformatics.dev/api/movement.plots.plot_centroid_trajectory.html)

:::

::: {.content-visible when-profile="answers"}

### Solutions: Exercise A

Loading pose tracks from a file:

```{python}
from pathlib import Path
from movement.io import load_poses

file_name = "CalMS21_mouse044_task1_annotator1.slp"
file_path = Path.home() / ".movement" / "CalMS21" / file_name

ds = load_poses.from_file(file_path, source_software="SLEAP", fps=30)
ds
```

Computing the minimum and maximum x,y positions:

```{python}
for space_coord in ["x", "y"]:
    min_pos = ds.position.sel(space=space_coord).min().values
    max_pos = ds.position.sel(space=space_coord).max().values
    print(f"Minimum {space_coord} position: {min_pos}")
    print(f"Maximum {space_coord} position: {max_pos}")
```

Plotting the centroid trajectory:

```{python}
from movement.plots import plot_centroid_trajectory

plot_centroid_trajectory(ds.position, individual="resident_b")
```

As a __bonus__, we can also overlay that trajectory on top of a video frame.

```{python}
import sleap_io as sio


video_path = Path.home() / ".movement" / "CalMS21" / "mouse044_task1_annotator1.mp4"
video = sio.load_video(video_path)

n_frames, height, width, channels = video.shape
print(f"Number of frames: {n_frames}")
print(f"Frame size: {width}x{height}")
print(f"Number of channels: {channels}\n")

# Extract the first frame to use as background
background = video[0]

fig, ax = plt.subplots()

# Plot the first video frame
ax.imshow(background, cmap="gray")

# Plot the centroid trajectory
plot_centroid_trajectory(
    ds.position, individual="resident_b", ax=ax, alpha=0.75, s=5,
)

plt.show()
```

Plot x,y positions of a certain keypoint across time, within a narrow time window:

```{python}
ds.position.sel(keypoints="tail_base", time=slice(0, 10)).plot.line(
    x="time", row="individuals", hue="space", aspect=2, size=2.5
)
```

Save a narrow time window dataset to a netCDF file:

```{.python}
ds.sel(time=slice(0, 10)).to_netcdf("narrow_time_window.nc")
```

:::

### Exploring data in the GUI

The `movement` graphical user interface (GUI), powered by our custom plugin for [napari](https://napari.org/dev/), makes it easy to view and explore movement motion tracks. Currently, you can use it to visualise 2D `movement` datasets as points, tracks, and rectangular bounding boxes (if defined) overlaid on video frames.

Let's go through the [Graphical User Interface section](https://movement.neuroinformatics.dev/user_guide/gui.html) of the `movement` documentation to see how to use it. Feel free to follow along using the `CalMS21` dataset you've also used for Exercise A, or any other tracking data from our [supported third-party formats](https://movement.neuroinformatics.dev/user_guide/input_output.html#supported-formats).

![Data from the CalMS21 dataset viewed via the `movement` plugin for `napari`](img/napari_calms21_screenshot.png){#fig-napari-calms21}
