# A mouse's daily activity log {#sec-movement-mouse}

In this case study, we will use the `movement` package to analyse mouse home cage monitoring data acquired in [Smart-Kages](https://cambridgephenotyping.com/products) and tracked with [DeepLabCut](https://www.deeplabcut.org/). Specifically, we'll look at how the mouse's activity levels vary during the day.

Before we start, make sure you have created the `animals-in-motion-env` environment (see [prerequisites @sec-install-movement]), and are using it to run this notebook.
Also, make sure you've downloaded the `Smart-Kages.zip` archive from [Dropbox](https://www.dropbox.com/scl/fo/81ug5hoy9msc7v7bteqa0/AH32RLdbZqWZJstIeR4YHZY?rlkey=blgagtaizw8aac5areja6h7q1&st=w1zueyi9&dl=0) (see [prerequisites @sec-data]) and unzipped it.

## Import libraries

```{python}
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import xarray as xr

from movement import sample_data
from movement.filtering import filter_by_confidence
from movement.kinematics import compute_speed
from movement.plots import plot_occupancy

```

```{python}
#| include: false

xr.set_options(
    display_expand_attrs=False,
    display_expand_coords=False,
    keep_attrs=True,
)
```

## The Smart-Kages dataset

::: {.callout-note title="Acknowledgement"}
This dataset was kindly shared by [Loukia Katsouri](https://www.sainsburywellcome.org/web/people/loukia-katsouri) from the [O'Keefe Lab](https://www.sainsburywellcome.org/web/groups/okeefe-lab), with permission to use for this workshop.
:::

The Smart-Kages dataset contains home cage recordings from two mice,
each housed in a specialised [Smart-Kage](https://cambridgephenotyping.com/products) [@ho_fully_2023]â€”a home cage
monitoring system that includes a camera attached to the top of the cage.

The system acquires video frames at 2 frames per second and saves a video
segment for each hour of the day. A pre-trained DeepLabCut model is then
used to predict 8 keypoints on the mouse's body.

Let's see what's in the downloaded data.
You will need to specify the path to the unzipped `Smart-Kages` folder on your machine.

```{python}
# Replace with the path to the unzipped Smart-Kages folder on your machine
smart_kages_path = Path.home() / ".movement" / "Smart-Kages"

# Let's visualise the contents of the folder
files = [f.name for f in smart_kages_path.iterdir()]
files.sort()
for file in files:
    print(file)
```

The tracking data are stored in two`.nc` (NetCDF) files (one per mouse): `kage14` and `kage17`.
NetCDF is an HDF5-based file format that can be natively saved/loaded by the `xarray` library,
and is therefore [convenient to use with `movement`](https://movement.neuroinformatics.dev/user_guide/input_output.html#native-saving-and-loading-with-netcdf).

Apart from these, we also have two `.png` files: `kage14_background.png` and `kage17_background.png`,
which constitute frames extracted from the videos.

Let's take a look at them.

```{python}
#| label: fig-background-frames
#| fig-cap: "Top-down camera views of the Smart-Kage habitats"
#| code-fold: true

kages = ["kage14", "kage17"]
img_paths = [smart_kages_path / f"{kage}_background.png" for kage in kages]

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))

for i, img_path in enumerate(img_paths):
    img = plt.imread(img_path)
    axes[i].imshow(img)
    axes[i].set_title(f"{kages[i]}")
    axes[i].axis("off")

```

::: {.callout-tip title="Question"}
Now that you've seen these habitats:

What challenges do you anticipate with tracking a mouse in this type of environment?
:::

Let's load and inspect the tracking data:


```{python}
ds_kages = {}

for kage in ["kage14", "kage17"]:
    ds_kages[kage] = xr.open_dataset(smart_kages_path / f"{kage}.nc")

ds_kages["kage14"]   # Change to "kage17" to inspect the other dataset
```

We see that each dataset contains a huge amount of data!

```{python}
#| code-fold: true

start_date_k14 = pd.to_datetime(ds_kages["kage14"].time.data[0])
end_date_k14 = pd.to_datetime(ds_kages["kage14"].time.data[-1])
duration_k14 = end_date_k14 - start_date_k14

start_date_k17 = pd.to_datetime(ds_kages["kage17"].time.data[0])
end_date_k17 = pd.to_datetime(ds_kages["kage17"].time.data[-1])
duration_k17 = end_date_k17 - start_date_k17

print("Experiment durations:")
print(f"kage-14: from {start_date_k14} to {end_date_k14} ({duration_k14.days} days)")
print(f"kage-17: from {start_date_k17} to {end_date_k17} ({duration_k17.days} days)")
```

## Datetime coordinates

The time coordinates are given in seconds since the start of each video segment,
i.e. the coordinates represent "elapsed time" since the start of the video segment.

We would like to concatenate the datasets along the time dimension, so that we
can work with the data as a single time series.
However, if we do this naively, we will get a dataset with a time dimension
that goes back to 0 every time a new segment starts.

Ideally, we want a continuously increasing time dimension, expressed in
"calendar time.

The following code block assigns datetime coordinates to each dataset,
using the start datetime of each video segment as the reference point
and assuming a constant frame rate.


For some computations, it's still useful to also know the total time elapsed
since the start of the experiment. Therefore, we'll assign a secondary
time coordinate to the dataset, called `seconds_elapsed`, to store
that information.

```{.python}
first_segment_start = ds_3hr.time.isel(time=0).data
seconds_since_start = (
    ds_3hr.time.data - np.datetime64(first_segment_start)
) / pd.Timedelta("1s")
ds_3hr = ds_3hr.assign_coords(seconds_elapsed=("time", seconds_since_start))

print(f"First 5 'time' coords: {ds_3hr.coords["time"].values[:5]}")
print(f"First 5 'seconds_elapsed' coords: {ds_3hr.coords["seconds_elapsed"].values[:5]}")
```

This is quite convenient, because we can now select time windows in
various ways.

```{.python}
# 1. Select a time window by frame number.
ds_3hr.isel(time=slice(100, 200))

# 2. Select a time window by datetime.
ds_3hr.sel(time=slice("2024-04-17 09:30:00", "2024-04-17 10:00:00"))

# 3. Select a time window by seconds elapsed since the start
#    (we have to change the time coordinates to "seconds_elapsed" first).
ds_3hr.set_index(time="seconds_elapsed").sel(time=slice(0, 1800))
```

## Filtering out low-confidence predictions

Let's examine the confidence histograms for each keypoint.

```{.python}
#| label: fig-confidence-histograms
#| fig-cap: "Confidence histograms by keypoint"
#| code-fold: true
#| code-summary: "Show the code"

def plot_confidence_hist_by_keypoint(
    ds: xr.Dataset,
    save_path: Path | None = None,
) -> None:
    """
    Plot histograms of confidence values for each keypoint in the dataset.

    Parameters
    ----------
    ds : xr.Dataset
        The dataset containing keypoint positions and confidence values.
    save_path : Path | None
        Optional path to save the plot. If None, the plot will not be saved.
    """
    n_keypoints = ds.sizes["keypoints"]

    # Create subplots for each keypoint
    fig, axes = plt.subplots(
        nrows=2,
        ncols=(n_keypoints + 1) // 2,
        figsize=(n_keypoints * 1.5, n_keypoints * 0.75),
        sharey=True,
        sharex=True,
    )

    # Loop through each keypoint and plot its confidence histogram
    for i, kpt in enumerate(ds.keypoints.values):
        ax = axes[i % 2, i // 2]
        ds.confidence.sel(keypoints=kpt).plot.hist(
            bins=20,
            ax=ax,
            label=kpt,
            histtype="stepfilled",
            density=True,
        )
        ax.set_ylabel("Density")
        ax.set_xlabel("")
        ax.set_xlabel("Confidence")
        ax.set_title(kpt)

    plt.suptitle("Confidence Histograms by Keypoint")
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=128)


plot_confidence_hist_by_keypoint(ds_3hr)
```

It looks like the "neck", "bodycenter", "spine1", and "spine2" keypoints
are the most confidently detected.

Let's run the numbers:

```{.python}
#| label: fig-median-confidence
#| fig-cap: "Median confidence by keypoint"

fig, ax = plt.subplots(figsize=(7, 3))
ds_3hr.confidence.median(dim="time").squeeze().plot.line("o-", ax=ax)
ax.set_title("Median confidence by keypoint")
plt.show()
```

We can filter out low-confidence predictions.

```{.python}
confidence_threshold = 0.5

ds_3hr["position_filtered"] = filter_by_confidence(
    ds_3hr.position,
    ds_3hr.confidence,
    threshold=confidence_threshold,
    print_report=True,
)
```

Let us define a list of "reliable" keypoints for later use.
These are all on the mouse's body.

```{.python}
reliable_keypoints = ["neck", "bodycenter", "spine1", "spine2"]
```

## Plot the speed of the mouse over time

Let's compute the centroid of the 4 reliable body keypoints

```{.python}
body_centroid = ds_3hr.position_filtered.sel(
    individuals="individual_0",  # the only individual in the dataset
    keypoints=reliable_keypoints
).mean(dim="keypoints")
```

Now let's the body centroid speed as a proxy of the mouse's speed.

```{.python}
body_speed = compute_speed(body_centroid.set_index(time="seconds_elapsed"))
body_speed = body_speed.assign_coords(time=body_centroid.time)
```

Let's plot the speed over time.

```{.python}
#| label: fig-body-speed
#| fig-cap: "Body centroid speed"

fig, ax = plt.subplots(figsize=(10, 3))
body_speed.plot.line(ax=ax)
ax.set_title("Body centroid speed (pixels per second)")
plt.show()
```

## Plot the occupancy heatmap

For the heatamp calculation to properly work, we need to temporarily
set the time coordinates to "elapsed time".

```{.python}
fig, ax = plt.subplots()

height, width = example_frame.shape[:2]

ax.imshow(example_frame)
plot_occupancy(
    ds_3hr.position_filtered.set_index(time="seconds_elapsed"),
    keypoints=reliable_keypoints,
    ax=ax,
    cmap="turbo",
    norm="log",  # log scale the colormap
    cmin=0,      # only show occupancy above this number of frames
    alpha=0.6,   # some transparency
)
# invert y-axis to match the video frame
ax.set_ylim([height - 1, 0])
ax.set_xlim([0, width])
ax.set_title(f"Body centroid occupancy (log scale)")
```
